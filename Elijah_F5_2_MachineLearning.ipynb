{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Machine Learning with Text Data\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F5_2_MachineLearning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "SLP: Vector Semantics and Embeddings, Chapter 6 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "scikit-learn API reference: https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: sklearn in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (0.0.post9)\n",
      "Requirement already satisfied: keras in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: tensorflow in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: transformers in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: packaging in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: filelock in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets sklearn keras tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: A typical machine learning setup\n",
    "\n",
    "Source: https://github.com/merriekay/S23-CS167-Notes/blob/main/Day11_Scikit_Learn_Practice.ipynb\n",
    "\n",
    "**Discussion Question:** What are `train_data` and `train_sln` in `dt.fit(train_data,train_sln)`?\n",
    "* What shapes do they have?\n",
    "* What data types are involved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "___PREDICTED___ \t  ___ACTUAL___\n",
      "Iris-versicolor \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-virginica \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "-------------------------------------------------------\n",
      "                 Iris-setosa  Iris-versicolor  Iris-virginica\n",
      "Iris-setosa                9                0               0\n",
      "Iris-versicolor            0               10               1\n",
      "Iris-virginica             0                2               8\n"
     ]
    }
   ],
   "source": [
    "#classic scikit-learn algorithm\n",
    "\n",
    "#0. import libraries\n",
    "import sklearn\n",
    "import pandas\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "\n",
    "#1. load data\n",
    "#path = 'datasets/irisData.csv' #'/content/drive/MyDrive/CS167/datasets/irisData.csv'\n",
    "path = \"https://raw.githubusercontent.com/ericmanley/F23-CS195NLP/main/data/irisData.csv\"\n",
    "iris_data = pandas.read_csv(path)\n",
    "\n",
    "#2. split data\n",
    "predictors = ['sepal length', 'sepal width','petal length', 'petal width']\n",
    "target = \"species\"\n",
    "train_data, test_data, train_sln, test_sln = \\\n",
    "        train_test_split(iris_data[predictors], iris_data[target], test_size = 0.2, random_state=41)\n",
    "\n",
    "#3. Create classifier/regressor object (change these parameters for Exercise #1)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "#4. Call fit (to train the classification/regression model)\n",
    "dt.fit(train_data,train_sln)\n",
    "\n",
    "#5. Call predict to generate predictions\n",
    "iris_predictions = dt.predict(test_data)\n",
    "\n",
    "#6. Call a metric function to measure performance\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test_sln,iris_predictions))\n",
    "\n",
    "# Show the acutal and predicted (this isn't necessary, but may help catch bugs)\n",
    "print(\"___PREDICTED___ \\t  ___ACTUAL___\")\n",
    "for i in range(len(test_sln)):\n",
    "    print(iris_predictions[i],\"\\t\\t\", test_sln.iloc[i])\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "#print out a confusion matrix\n",
    "iris_labels= [\"Iris-setosa\", \"Iris-versicolor\",\"Iris-virginica\"]\n",
    "conf_mat = metrics.confusion_matrix(test_sln, iris_predictions, labels=iris_labels)\n",
    "print(pandas.DataFrame(conf_mat,index = iris_labels, columns = iris_labels))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making it work for text data\n",
    "\n",
    "To prepare your text data, you'll need to\n",
    "* tokenize\n",
    "* encode (get a numerical representation)\n",
    "\n",
    "Let's practice preparing a Hugging Face dataset for spam detection.\n",
    "\n",
    "https://huggingface.co/datasets/Deysi/spam-detection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8175\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at a couple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get rich quick!!! Make $$$ in just 30 minutes, GUARANTEED! No skills or experience required! Just send me a small fee and I'll give you access to my exclusive money-making secrets!\n",
      "\n",
      "\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "print( data[\"train\"][\"text\"][32] )\n",
    "\n",
    "print( data[\"train\"][\"label\"][32] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time poster in this sub, and I'm only an amateur with data compared to many.\n",
      "\n",
      "But, want I want to do is figure out the partisan lean for each of the new state house districts in Texas. This type of analysis is easily available for the federal districts at sites like 538 ( https://projects.fivethirtyeight.com/redistricting-2022-maps ), but I want to do it at the state house level.\n",
      "\n",
      "Then, as a second step, it'd be great to have the correct data to draw my own state congressional districts and derive partisan leans (as well as determine the racial makeup). But, this would be a much larger project for the future.\n",
      "\n",
      "Anyway, I'm struggling to figure out where to even start. I could get precinct data from the MIT data lab, but state districts break up even precincts, so they wouldn't be totally accurate.\n",
      "\n",
      "Thanks in advance for any thoughts or help!\n",
      "not_spam\n"
     ]
    }
   ],
   "source": [
    "print( data[\"train\"][\"text\"][35] )\n",
    "\n",
    "print( data[\"train\"][\"label\"][35] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "This could be done with any of the tokenization methods we've looked at.\n",
    "\n",
    "Let's try it with the WordPiece algorithm (used by the BERT models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'poster', 'in', 'this', 'sub', ',', 'and', 'i', \"'\", 'm', 'only', 'an', 'amateur', 'with', 'data', 'compared', 'to', 'many', '.', 'but', ',', 'want', 'i', 'want', 'to', 'do', 'is', 'figure', 'out', 'the', 'partisan', 'lean', 'for', 'each', 'of', 'the', 'new', 'state', 'house', 'districts', 'in', 'texas', '.', 'this', 'type', 'of', 'analysis', 'is', 'easily', 'available', 'for', 'the', 'federal', 'districts', 'at', 'sites', 'like', '53', '##8', '(', 'https', ':', '/', '/', 'projects', '.', 'five', '##thi', '##rt', '##ye', '##ight', '.', 'com', '/', 'red', '##ist', '##ricting', '-', '202', '##2', '-', 'maps', ')', ',', 'but', 'i', 'want', 'to', 'do', 'it', 'at', 'the', 'state', 'house', 'level', '.', 'then', ',', 'as', 'a', 'second', 'step', ',', 'it', \"'\", 'd', 'be', 'great', 'to', 'have', 'the', 'correct', 'data', 'to', 'draw', 'my', 'own', 'state', 'congressional', 'districts', 'and', 'derive', 'partisan', 'leans', '(', 'as', 'well', 'as', 'determine', 'the', 'racial', 'makeup', ')', '.', 'but', ',', 'this', 'would', 'be', 'a', 'much', 'larger', 'project', 'for', 'the', 'future', '.', 'anyway', ',', 'i', \"'\", 'm', 'struggling', 'to', 'figure', 'out', 'where', 'to', 'even', 'start', '.', 'i', 'could', 'get', 'precinct', 'data', 'from', 'the', 'mit', 'data', 'lab', ',', 'but', 'state', 'districts', 'break', 'up', 'even', 'precinct', '##s', ',', 'so', 'they', 'wouldn', \"'\", 't', 'be', 'totally', 'accurate', '.', 'thanks', 'in', 'advance', 'for', 'any', 'thoughts', 'or', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer.tokenize(data[\"train\"][\"text\"][35])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Integer Encodings\n",
    "\n",
    "The Hugging Face tokenizers assign a numerical ID to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2034,\n",
       " 2051,\n",
       " 13082,\n",
       " 1999,\n",
       " 2023,\n",
       " 4942,\n",
       " 1010,\n",
       " 1998,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 2069,\n",
       " 2019,\n",
       " 5515,\n",
       " 2007,\n",
       " 2951,\n",
       " 4102,\n",
       " 2000,\n",
       " 2116,\n",
       " 1012,\n",
       " 2021,\n",
       " 1010,\n",
       " 2215,\n",
       " 1045,\n",
       " 2215,\n",
       " 2000,\n",
       " 2079,\n",
       " 2003,\n",
       " 3275,\n",
       " 2041,\n",
       " 1996,\n",
       " 14254,\n",
       " 8155,\n",
       " 2005,\n",
       " 2169,\n",
       " 1997,\n",
       " 1996,\n",
       " 2047,\n",
       " 2110,\n",
       " 2160,\n",
       " 4733,\n",
       " 1999,\n",
       " 3146,\n",
       " 1012,\n",
       " 2023,\n",
       " 2828,\n",
       " 1997,\n",
       " 4106,\n",
       " 2003,\n",
       " 4089,\n",
       " 2800,\n",
       " 2005,\n",
       " 1996,\n",
       " 2976,\n",
       " 4733,\n",
       " 2012,\n",
       " 4573,\n",
       " 2066,\n",
       " 5187,\n",
       " 2620,\n",
       " 1006,\n",
       " 16770,\n",
       " 1024,\n",
       " 1013,\n",
       " 1013,\n",
       " 3934,\n",
       " 1012,\n",
       " 2274,\n",
       " 15222,\n",
       " 5339,\n",
       " 6672,\n",
       " 18743,\n",
       " 1012,\n",
       " 4012,\n",
       " 1013,\n",
       " 2417,\n",
       " 2923,\n",
       " 28827,\n",
       " 1011,\n",
       " 16798,\n",
       " 2475,\n",
       " 1011,\n",
       " 7341,\n",
       " 1007,\n",
       " 1010,\n",
       " 2021,\n",
       " 1045,\n",
       " 2215,\n",
       " 2000,\n",
       " 2079,\n",
       " 2009,\n",
       " 2012,\n",
       " 1996,\n",
       " 2110,\n",
       " 2160,\n",
       " 2504,\n",
       " 1012,\n",
       " 2059,\n",
       " 1010,\n",
       " 2004,\n",
       " 1037,\n",
       " 2117,\n",
       " 3357,\n",
       " 1010,\n",
       " 2009,\n",
       " 1005,\n",
       " 1040,\n",
       " 2022,\n",
       " 2307,\n",
       " 2000,\n",
       " 2031,\n",
       " 1996,\n",
       " 6149,\n",
       " 2951,\n",
       " 2000,\n",
       " 4009,\n",
       " 2026,\n",
       " 2219,\n",
       " 2110,\n",
       " 7740,\n",
       " 4733,\n",
       " 1998,\n",
       " 18547,\n",
       " 14254,\n",
       " 12671,\n",
       " 1006,\n",
       " 2004,\n",
       " 2092,\n",
       " 2004,\n",
       " 5646,\n",
       " 1996,\n",
       " 5762,\n",
       " 5789,\n",
       " 1007,\n",
       " 1012,\n",
       " 2021,\n",
       " 1010,\n",
       " 2023,\n",
       " 2052,\n",
       " 2022,\n",
       " 1037,\n",
       " 2172,\n",
       " 3469,\n",
       " 2622,\n",
       " 2005,\n",
       " 1996,\n",
       " 2925,\n",
       " 1012,\n",
       " 4312,\n",
       " 1010,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 8084,\n",
       " 2000,\n",
       " 3275,\n",
       " 2041,\n",
       " 2073,\n",
       " 2000,\n",
       " 2130,\n",
       " 2707,\n",
       " 1012,\n",
       " 1045,\n",
       " 2071,\n",
       " 2131,\n",
       " 18761,\n",
       " 2951,\n",
       " 2013,\n",
       " 1996,\n",
       " 10210,\n",
       " 2951,\n",
       " 6845,\n",
       " 1010,\n",
       " 2021,\n",
       " 2110,\n",
       " 4733,\n",
       " 3338,\n",
       " 2039,\n",
       " 2130,\n",
       " 18761,\n",
       " 2015,\n",
       " 1010,\n",
       " 2061,\n",
       " 2027,\n",
       " 2876,\n",
       " 1005,\n",
       " 1056,\n",
       " 2022,\n",
       " 6135,\n",
       " 8321,\n",
       " 1012,\n",
       " 4283,\n",
       " 1999,\n",
       " 5083,\n",
       " 2005,\n",
       " 2151,\n",
       " 4301,\n",
       " 2030,\n",
       " 2393,\n",
       " 999]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's get the encodings for all the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2131, 4138, 4248, 999, 999, 999, 2191, 1002, 1002, 1002, 1999, 2074, 2382, 2781, 1010, 12361, 999, 2053, 4813, 2030, 3325, 3223, 999, 2074, 4604, 2033, 1037, 2235, 7408, 1998, 1045, 1005, 2222, 2507, 2017, 3229, 2000, 2026, 7262, 2769, 1011, 2437, 7800, 999], [2131, 4138, 4248, 1998, 3733, 2007, 16021, 2696, 15671, 2232, 999, 999, 2024, 2017, 5458, 1997, 2108, 3631, 1998, 2542, 3477, 5403, 3600, 2000, 3477, 5403, 3600, 1029, 2079, 2017, 2215, 2000, 2191, 2070, 3733, 2769, 2302, 5128, 1999, 2151, 2613, 3947, 1029, 2092, 1010, 2298, 2053, 2582, 2138, 16021, 2696, 15671, 2232, 2038, 2288, 2017, 3139, 999, 2007, 2256, 6208, 2291, 1010, 2017, 2064, 2191, 2039, 2000, 1002, 6694, 1037, 2154, 2074, 2011, 16663, 1010, 6631, 1010, 1998, 15591, 2006, 8466, 1012, 2009, 1005, 1055, 2008, 3722, 999, 2053, 4813, 2030, 3325, 4072, 1010, 2074, 1037, 26381, 1998, 2019, 4274, 4434, 1012, 1998, 1996, 2190, 2112, 2003, 1010, 2017, 2064, 2707, 7414, 3202, 1012, 2053, 3403, 2005, 6226, 2015, 2030, 8552, 3696, 1011, 2039, 6194, 1012, 2074, 8816, 1996, 16021, 2696, 15671, 2232, 10439, 1998], [4931, 2045, 999, 2024, 2017, 5458, 1997, 3110, 2894, 1998, 23657, 1029, 2079, 2017, 2215, 2000, 7532, 2007, 2500, 2040, 3745, 2115, 5426, 1998, 25289, 1029, 2298, 2053, 2582, 2084, 2256, 6429, 2591, 2897, 999, 2256, 4132, 2003, 1996, 3819, 2173, 2000, 12403, 2213, 2115, 2814, 2007, 7696, 1998, 14389, 1010, 2191, 8275, 6115, 1010, 1998, 3745, 2659, 1011, 3737, 2033, 7834, 1012, 2057, 2031, 5717, 4781, 2005, 4180, 3737, 2030, 5310, 5248, 1010, 2061, 2017, 2064, 2514, 2489, 2000, 2022, 2004, 27885, 3630, 25171, 1998, 23217, 3512, 2004, 2017, 2215, 1012, 4606, 1010, 2057, 3749, 6197, 1997, 2838, 2008, 2017, 1005, 2222, 2196, 2224, 1010, 2107, 2004, 23100, 19461, 11254, 1010, 11809, 14592, 1010, 1998, 15703, 26828, 2015, 2008, 2017, 2064, 1005, 1056, 2735, 2125, 1012, 2017, 1005, 2222, 2036, 2293, 2256, 5377, 10400, 1997, 14997]]\n"
     ]
    }
   ],
   "source": [
    "encodings = []\n",
    "for curr_example in data[\"train\"][\"text\"]:\n",
    "    curr_tokens = tokenizer.tokenize(curr_example)\n",
    "    curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "    encodings.append(curr_encodings)\n",
    "        \n",
    "#display some of them\n",
    "print(encodings[32:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the warning is telling you that the data set has token sequences that are too big for the BERT model - it'll still produce the correct tokens, though, and we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Discussion Question\n",
    "\n",
    "The hope is that we could now use the encodings as the predictors and fit a model to them like this. \n",
    "\n",
    "But this doesn't work - what is the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings)==len(data[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8175,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dt\u001b[39m.\u001b[39;49mfit(encodings,data[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    960\u001b[0m         X,\n\u001b[1;32m    961\u001b[0m         y,\n\u001b[1;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    238\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    239\u001b[0m     dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, force_all_finite\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 242\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    243\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m missing_values_in_feature_mask \u001b[39m=\u001b[39m (\n\u001b[1;32m    247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/base.py:617\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[1;32m    616\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 617\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_X_params)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[1;32m    619\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8175,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "dt.fit(encodings,data[\"train\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the problem is that the data is in the wrong shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726605504587156\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "\n",
    "def prepare_text_data(text_list,tokenizer,encoded_length=512):\n",
    "    encodings = []\n",
    "    for curr_example in text_list:\n",
    "        curr_tokens = tokenizer.tokenize(curr_example)\n",
    "        curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "        # truncate sequences that are too long\n",
    "        if len(curr_encodings) > encoded_length:\n",
    "            curr_encodings = curr_encodings[:encoded_length]\n",
    "        # pad sequences that are too short with 0s\n",
    "        elif len(curr_encodings) < encoded_length:\n",
    "            curr_encodings = curr_encodings + [0]*(encoded_length-len(curr_encodings))\n",
    "\n",
    "        encodings.append(curr_encodings)\n",
    "        \n",
    "    return encodings\n",
    "\n",
    "\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Group Discussion\n",
    "\n",
    "Is this a good accuracy score?\n",
    "\n",
    "What context do we need to interpret this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark style=\"background: lightgreen\"> It's okay but could use improvement, if you look at how it is split on the classes it is about 50% so 72% is better than guessing or choosing spam from all predictions</mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9368807339449541\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = RandomForestClassifier()\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['id', 'text', 'label', 'dataset', 'license', 'language']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# load the first 1000 of dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mma2za/many_emotions\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain[:1000]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_encoding \u001b[39m=\u001b[39m prepare_text_data(data[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_labels \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_encoding \u001b[39m=\u001b[39m prepare_text_data(data[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],tokenizer)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:2787\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2786\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2787\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2788\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2790\u001b[0m )\n\u001b[1;32m   2791\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/datasets/formatting/formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 580\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39;49mcolumn_names)\n\u001b[1;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/datasets/formatting/formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_column_key\u001b[39m(key: \u001b[39mstr\u001b[39m, columns: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n\u001b[0;32m--> 520\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['id', 'text', 'label', 'dataset', 'license', 'language']\""
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# load the first 1000 of dataset\n",
    "data = load_dataset(\"ma2za/many_emotions\", split=\"train[:1000]\")\n",
    "\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = RandomForestClassifier()\n",
    "\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Group Exercise\n",
    "\n",
    "Have everyone at your table agree on another classification model that you've used before. \n",
    "\n",
    "Run this code but with the other model. How do the results compare?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Exercise\n",
    "\n",
    "Pick a different *text classification* dataset and test the accuracy of this approach on that data. How do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with this encoding\n",
    "\n",
    "The numerical encoding we've used may not work well in many situations.\n",
    "\n",
    "Think about the predictor features our model is using\n",
    "\n",
    "`['first', 'time', 'poster', 'in', 'this', 'sub',`\n",
    "\n",
    "`[2034, 2051, 13082, 1999, 2023, 4942,`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first feature is \"first token in the sequence\"\n",
    "\n",
    "2nd feature is \"second token in the sequence\"\n",
    "\n",
    "etc.\n",
    "\n",
    "If a word appears in different positions in two different sequences, the model won't inherently recognize that it's the same word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The way words get mapped to numbers is arbitrary - there's no relationship between words that have close numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2034]\n",
      "[2051]\n"
     ]
    }
   ],
   "source": [
    "print( tokenizer.encode(\"first\",add_special_tokens=False) )\n",
    "print( tokenizer.encode(\"time\",add_special_tokens=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-Words Encoding\n",
    "\n",
    "Choose vocabulary (say 5000 most common words) one column for each word\n",
    "\n",
    "row contains counts for each word\n",
    "\n",
    "**Example**\n",
    "\n",
    "*Sentence 1:* \"The cat sat on the hat\"\n",
    "\n",
    "*Sentence 2:* \"The dog ate the cat and the hat\" \n",
    "\n",
    "*Vocabulary:* { the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "\n",
    "|            | the | cat | sat | on | hat | dog | ate | and |\n",
    "|------------|-----|-----|-----|----|-----|-----|-----|-----|\n",
    "| Sentence 1 | 2   | 1   | 1   | 1  | 1   | 0   | 0   | 0   |\n",
    "| Sentence 2 | 3   | 1   | 0   | 0  | 1   | 1   | 1   | 1   |\n",
    "\n",
    "\n",
    "**The downside:** this doesn't maintain any information about word order - thus the \"bag\" of words\n",
    "\n",
    "`scikit-learn` provides a Bag-of-Words encoder called `CountVectorizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "# remove stop words\n",
    "vectorizer = CountVectorizer(max_features=5000,stop_words=\"english\")  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TD-IDF Encoding\n",
    "\n",
    "**TF-IDF:** Term Frequency - Inverse Document Frequency\n",
    "\n",
    "**Term Frequency:** How often does the word appear in the example, like CountVectorizer\n",
    "* actually take the $\\log$ of it\n",
    "\n",
    "**Document Frequency:** What fraction of the *documents* (or *training-examples*) does the word appear in?\n",
    "\n",
    "**Inverse Document Frequency:** (number of documents) / (number of documents containing the word)\n",
    "* if a word is in only a few documents, you get a big number\n",
    "* if a word appears in lots of documents, you get a small number\n",
    "\n",
    "When encoding a new example, multiply the Term Frequency of the word in this example by the Inverse Document Frequency of the training set\n",
    "* gives higher weight to words that are differentiators\n",
    "* stop words should automatically be de-emphasized\n",
    "\n",
    "**Example:**\n",
    "Document collection: all of Shakespeare's plays\n",
    "\n",
    "The word `Romeo` appears 113 times but only in 1 document\n",
    "\n",
    "The word `action` appears 113 time but in 31 documents \n",
    "\n",
    "so Romeo will get a much higher weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Try the Integer Encoding, Count Vectorizer, and TD-IDF on a *new dataset*\n",
    "* use at least three different classification models\n",
    "\n",
    "Give a short write-up on the following\n",
    "* Describe your dataset, including the distribution of the target variable\n",
    "* Describe the results of the machine learning experiment\n",
    "* Interpret the results - why do you think each of the encodings and algorithms performed the way they did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## if time....\n",
    "\n",
    "Here's some code for building a multilayer neural network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 0s 808us/step - loss: 0.2739 - accuracy: 0.9697\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 805us/step - loss: 0.0388 - accuracy: 0.9989\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 796us/step - loss: 0.0156 - accuracy: 0.9991\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 772us/step - loss: 0.0088 - accuracy: 0.9993\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 773us/step - loss: 0.0058 - accuracy: 0.9995\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 771us/step - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 765us/step - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 816us/step - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 830us/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 783us/step - loss: 0.0016 - accuracy: 0.9996\n",
      "86/86 [==============================] - 0s 535us/step - loss: 0.0052 - accuracy: 0.9985\n",
      "Test accuracy: 99.85%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# the sklearn vectors need to be converted to numpy arrays for this library\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays = test_vectors.toarray()\n",
    "# Convert string labels to binary labels\n",
    "train_labels_array = np.array([1 if label == \"spam\" else 0 for label in train_labels])\n",
    "test_labels_array = np.array([1 if label == \"spam\" else 0 for label in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vectors_arrays, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
