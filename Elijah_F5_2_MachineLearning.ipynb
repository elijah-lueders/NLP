{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Machine Learning with Text Data\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F5_2_MachineLearning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "SLP: Vector Semantics and Embeddings, Chapter 6 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "scikit-learn API reference: https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: sklearn in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (0.0.post9)\n",
      "Requirement already satisfied: keras in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: transformers in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: packaging in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-1.24.3-cp310-cp310-macosx_11_0_arm64.whl (13.9 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Collecting keras\n",
      "  Obtaining dependency information for keras from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, numpy, keras\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.5\n",
      "    Uninstalling urllib3-2.0.5:\n",
      "      Successfully uninstalled urllib3-2.0.5\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "Successfully installed keras-2.13.1 numpy-1.24.3 typing-extensions-4.5.0 urllib3-1.26.18\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets sklearn keras tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: A typical machine learning setup\n",
    "\n",
    "Source: https://github.com/merriekay/S23-CS167-Notes/blob/main/Day11_Scikit_Learn_Practice.ipynb\n",
    "\n",
    "**Discussion Question:** What are `train_data` and `train_sln` in `dt.fit(train_data,train_sln)`?\n",
    "* What shapes do they have?\n",
    "* What data types are involved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "___PREDICTED___ \t  ___ACTUAL___\n",
      "Iris-versicolor \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-virginica \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "-------------------------------------------------------\n",
      "                 Iris-setosa  Iris-versicolor  Iris-virginica\n",
      "Iris-setosa                9                0               0\n",
      "Iris-versicolor            0               10               1\n",
      "Iris-virginica             0                2               8\n"
     ]
    }
   ],
   "source": [
    "#classic scikit-learn algorithm\n",
    "\n",
    "#0. import libraries\n",
    "import sklearn\n",
    "import pandas\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "\n",
    "#1. load data\n",
    "#path = 'datasets/irisData.csv' #'/content/drive/MyDrive/CS167/datasets/irisData.csv'\n",
    "path = \"https://raw.githubusercontent.com/ericmanley/F23-CS195NLP/main/data/irisData.csv\"\n",
    "iris_data = pandas.read_csv(path)\n",
    "\n",
    "#2. split data\n",
    "predictors = ['sepal length', 'sepal width','petal length', 'petal width']\n",
    "target = \"species\"\n",
    "train_data, test_data, train_sln, test_sln = \\\n",
    "        train_test_split(iris_data[predictors], iris_data[target], test_size = 0.2, random_state=41)\n",
    "\n",
    "#3. Create classifier/regressor object (change these parameters for Exercise #1)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "#4. Call fit (to train the classification/regression model)\n",
    "dt.fit(train_data,train_sln)\n",
    "\n",
    "#5. Call predict to generate predictions\n",
    "iris_predictions = dt.predict(test_data)\n",
    "\n",
    "#6. Call a metric function to measure performance\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test_sln,iris_predictions))\n",
    "\n",
    "# Show the acutal and predicted (this isn't necessary, but may help catch bugs)\n",
    "print(\"___PREDICTED___ \\t  ___ACTUAL___\")\n",
    "for i in range(len(test_sln)):\n",
    "    print(iris_predictions[i],\"\\t\\t\", test_sln.iloc[i])\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "#print out a confusion matrix\n",
    "iris_labels= [\"Iris-setosa\", \"Iris-versicolor\",\"Iris-virginica\"]\n",
    "conf_mat = metrics.confusion_matrix(test_sln, iris_predictions, labels=iris_labels)\n",
    "print(pandas.DataFrame(conf_mat,index = iris_labels, columns = iris_labels))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making it work for text data\n",
    "\n",
    "To prepare your text data, you'll need to\n",
    "* tokenize\n",
    "* encode (get a numerical representation)\n",
    "\n",
    "Let's practice preparing a Hugging Face dataset for spam detection.\n",
    "\n",
    "https://huggingface.co/datasets/Deysi/spam-detection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8175\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at a couple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get rich quick!!! Make $$$ in just 30 minutes, GUARANTEED! No skills or experience required! Just send me a small fee and I'll give you access to my exclusive money-making secrets!\n",
      "\n",
      "\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "print( data[\"train\"][\"text\"][32] )\n",
    "\n",
    "print( data[\"train\"][\"label\"][32] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time poster in this sub, and I'm only an amateur with data compared to many.\n",
      "\n",
      "But, want I want to do is figure out the partisan lean for each of the new state house districts in Texas. This type of analysis is easily available for the federal districts at sites like 538 ( https://projects.fivethirtyeight.com/redistricting-2022-maps ), but I want to do it at the state house level.\n",
      "\n",
      "Then, as a second step, it'd be great to have the correct data to draw my own state congressional districts and derive partisan leans (as well as determine the racial makeup). But, this would be a much larger project for the future.\n",
      "\n",
      "Anyway, I'm struggling to figure out where to even start. I could get precinct data from the MIT data lab, but state districts break up even precincts, so they wouldn't be totally accurate.\n",
      "\n",
      "Thanks in advance for any thoughts or help!\n",
      "not_spam\n"
     ]
    }
   ],
   "source": [
    "print( data[\"train\"][\"text\"][35] )\n",
    "\n",
    "print( data[\"train\"][\"label\"][35] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "This could be done with any of the tokenization methods we've looked at.\n",
    "\n",
    "Let's try it with the WordPiece algorithm (used by the BERT models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'poster', 'in', 'this', 'sub', ',', 'and', 'i', \"'\", 'm', 'only', 'an', 'amateur', 'with', 'data', 'compared', 'to', 'many', '.', 'but', ',', 'want', 'i', 'want', 'to', 'do', 'is', 'figure', 'out', 'the', 'partisan', 'lean', 'for', 'each', 'of', 'the', 'new', 'state', 'house', 'districts', 'in', 'texas', '.', 'this', 'type', 'of', 'analysis', 'is', 'easily', 'available', 'for', 'the', 'federal', 'districts', 'at', 'sites', 'like', '53', '##8', '(', 'https', ':', '/', '/', 'projects', '.', 'five', '##thi', '##rt', '##ye', '##ight', '.', 'com', '/', 'red', '##ist', '##ricting', '-', '202', '##2', '-', 'maps', ')', ',', 'but', 'i', 'want', 'to', 'do', 'it', 'at', 'the', 'state', 'house', 'level', '.', 'then', ',', 'as', 'a', 'second', 'step', ',', 'it', \"'\", 'd', 'be', 'great', 'to', 'have', 'the', 'correct', 'data', 'to', 'draw', 'my', 'own', 'state', 'congressional', 'districts', 'and', 'derive', 'partisan', 'leans', '(', 'as', 'well', 'as', 'determine', 'the', 'racial', 'makeup', ')', '.', 'but', ',', 'this', 'would', 'be', 'a', 'much', 'larger', 'project', 'for', 'the', 'future', '.', 'anyway', ',', 'i', \"'\", 'm', 'struggling', 'to', 'figure', 'out', 'where', 'to', 'even', 'start', '.', 'i', 'could', 'get', 'precinct', 'data', 'from', 'the', 'mit', 'data', 'lab', ',', 'but', 'state', 'districts', 'break', 'up', 'even', 'precinct', '##s', ',', 'so', 'they', 'wouldn', \"'\", 't', 'be', 'totally', 'accurate', '.', 'thanks', 'in', 'advance', 'for', 'any', 'thoughts', 'or', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer.tokenize(data[\"train\"][\"text\"][35])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Integer Encodings\n",
    "\n",
    "The Hugging Face tokenizers assign a numerical ID to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2034,\n",
       " 2051,\n",
       " 13082,\n",
       " 1999,\n",
       " 2023,\n",
       " 4942,\n",
       " 1010,\n",
       " 1998,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 2069,\n",
       " 2019,\n",
       " 5515,\n",
       " 2007,\n",
       " 2951,\n",
       " 4102,\n",
       " 2000,\n",
       " 2116,\n",
       " 1012,\n",
       " 2021,\n",
       " 1010,\n",
       " 2215,\n",
       " 1045,\n",
       " 2215,\n",
       " 2000,\n",
       " 2079,\n",
       " 2003,\n",
       " 3275,\n",
       " 2041,\n",
       " 1996,\n",
       " 14254,\n",
       " 8155,\n",
       " 2005,\n",
       " 2169,\n",
       " 1997,\n",
       " 1996,\n",
       " 2047,\n",
       " 2110,\n",
       " 2160,\n",
       " 4733,\n",
       " 1999,\n",
       " 3146,\n",
       " 1012,\n",
       " 2023,\n",
       " 2828,\n",
       " 1997,\n",
       " 4106,\n",
       " 2003,\n",
       " 4089,\n",
       " 2800,\n",
       " 2005,\n",
       " 1996,\n",
       " 2976,\n",
       " 4733,\n",
       " 2012,\n",
       " 4573,\n",
       " 2066,\n",
       " 5187,\n",
       " 2620,\n",
       " 1006,\n",
       " 16770,\n",
       " 1024,\n",
       " 1013,\n",
       " 1013,\n",
       " 3934,\n",
       " 1012,\n",
       " 2274,\n",
       " 15222,\n",
       " 5339,\n",
       " 6672,\n",
       " 18743,\n",
       " 1012,\n",
       " 4012,\n",
       " 1013,\n",
       " 2417,\n",
       " 2923,\n",
       " 28827,\n",
       " 1011,\n",
       " 16798,\n",
       " 2475,\n",
       " 1011,\n",
       " 7341,\n",
       " 1007,\n",
       " 1010,\n",
       " 2021,\n",
       " 1045,\n",
       " 2215,\n",
       " 2000,\n",
       " 2079,\n",
       " 2009,\n",
       " 2012,\n",
       " 1996,\n",
       " 2110,\n",
       " 2160,\n",
       " 2504,\n",
       " 1012,\n",
       " 2059,\n",
       " 1010,\n",
       " 2004,\n",
       " 1037,\n",
       " 2117,\n",
       " 3357,\n",
       " 1010,\n",
       " 2009,\n",
       " 1005,\n",
       " 1040,\n",
       " 2022,\n",
       " 2307,\n",
       " 2000,\n",
       " 2031,\n",
       " 1996,\n",
       " 6149,\n",
       " 2951,\n",
       " 2000,\n",
       " 4009,\n",
       " 2026,\n",
       " 2219,\n",
       " 2110,\n",
       " 7740,\n",
       " 4733,\n",
       " 1998,\n",
       " 18547,\n",
       " 14254,\n",
       " 12671,\n",
       " 1006,\n",
       " 2004,\n",
       " 2092,\n",
       " 2004,\n",
       " 5646,\n",
       " 1996,\n",
       " 5762,\n",
       " 5789,\n",
       " 1007,\n",
       " 1012,\n",
       " 2021,\n",
       " 1010,\n",
       " 2023,\n",
       " 2052,\n",
       " 2022,\n",
       " 1037,\n",
       " 2172,\n",
       " 3469,\n",
       " 2622,\n",
       " 2005,\n",
       " 1996,\n",
       " 2925,\n",
       " 1012,\n",
       " 4312,\n",
       " 1010,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 8084,\n",
       " 2000,\n",
       " 3275,\n",
       " 2041,\n",
       " 2073,\n",
       " 2000,\n",
       " 2130,\n",
       " 2707,\n",
       " 1012,\n",
       " 1045,\n",
       " 2071,\n",
       " 2131,\n",
       " 18761,\n",
       " 2951,\n",
       " 2013,\n",
       " 1996,\n",
       " 10210,\n",
       " 2951,\n",
       " 6845,\n",
       " 1010,\n",
       " 2021,\n",
       " 2110,\n",
       " 4733,\n",
       " 3338,\n",
       " 2039,\n",
       " 2130,\n",
       " 18761,\n",
       " 2015,\n",
       " 1010,\n",
       " 2061,\n",
       " 2027,\n",
       " 2876,\n",
       " 1005,\n",
       " 1056,\n",
       " 2022,\n",
       " 6135,\n",
       " 8321,\n",
       " 1012,\n",
       " 4283,\n",
       " 1999,\n",
       " 5083,\n",
       " 2005,\n",
       " 2151,\n",
       " 4301,\n",
       " 2030,\n",
       " 2393,\n",
       " 999]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's get the encodings for all the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2131, 4138, 4248, 999, 999, 999, 2191, 1002, 1002, 1002, 1999, 2074, 2382, 2781, 1010, 12361, 999, 2053, 4813, 2030, 3325, 3223, 999, 2074, 4604, 2033, 1037, 2235, 7408, 1998, 1045, 1005, 2222, 2507, 2017, 3229, 2000, 2026, 7262, 2769, 1011, 2437, 7800, 999], [2131, 4138, 4248, 1998, 3733, 2007, 16021, 2696, 15671, 2232, 999, 999, 2024, 2017, 5458, 1997, 2108, 3631, 1998, 2542, 3477, 5403, 3600, 2000, 3477, 5403, 3600, 1029, 2079, 2017, 2215, 2000, 2191, 2070, 3733, 2769, 2302, 5128, 1999, 2151, 2613, 3947, 1029, 2092, 1010, 2298, 2053, 2582, 2138, 16021, 2696, 15671, 2232, 2038, 2288, 2017, 3139, 999, 2007, 2256, 6208, 2291, 1010, 2017, 2064, 2191, 2039, 2000, 1002, 6694, 1037, 2154, 2074, 2011, 16663, 1010, 6631, 1010, 1998, 15591, 2006, 8466, 1012, 2009, 1005, 1055, 2008, 3722, 999, 2053, 4813, 2030, 3325, 4072, 1010, 2074, 1037, 26381, 1998, 2019, 4274, 4434, 1012, 1998, 1996, 2190, 2112, 2003, 1010, 2017, 2064, 2707, 7414, 3202, 1012, 2053, 3403, 2005, 6226, 2015, 2030, 8552, 3696, 1011, 2039, 6194, 1012, 2074, 8816, 1996, 16021, 2696, 15671, 2232, 10439, 1998], [4931, 2045, 999, 2024, 2017, 5458, 1997, 3110, 2894, 1998, 23657, 1029, 2079, 2017, 2215, 2000, 7532, 2007, 2500, 2040, 3745, 2115, 5426, 1998, 25289, 1029, 2298, 2053, 2582, 2084, 2256, 6429, 2591, 2897, 999, 2256, 4132, 2003, 1996, 3819, 2173, 2000, 12403, 2213, 2115, 2814, 2007, 7696, 1998, 14389, 1010, 2191, 8275, 6115, 1010, 1998, 3745, 2659, 1011, 3737, 2033, 7834, 1012, 2057, 2031, 5717, 4781, 2005, 4180, 3737, 2030, 5310, 5248, 1010, 2061, 2017, 2064, 2514, 2489, 2000, 2022, 2004, 27885, 3630, 25171, 1998, 23217, 3512, 2004, 2017, 2215, 1012, 4606, 1010, 2057, 3749, 6197, 1997, 2838, 2008, 2017, 1005, 2222, 2196, 2224, 1010, 2107, 2004, 23100, 19461, 11254, 1010, 11809, 14592, 1010, 1998, 15703, 26828, 2015, 2008, 2017, 2064, 1005, 1056, 2735, 2125, 1012, 2017, 1005, 2222, 2036, 2293, 2256, 5377, 10400, 1997, 14997]]\n"
     ]
    }
   ],
   "source": [
    "encodings = []\n",
    "for curr_example in data[\"train\"][\"text\"]:\n",
    "    curr_tokens = tokenizer.tokenize(curr_example)\n",
    "    curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "    encodings.append(curr_encodings)\n",
    "        \n",
    "#display some of them\n",
    "print(encodings[32:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the warning is telling you that the data set has token sequences that are too big for the BERT model - it'll still produce the correct tokens, though, and we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Discussion Question\n",
    "\n",
    "The hope is that we could now use the encodings as the predictors and fit a model to them like this. \n",
    "\n",
    "But this doesn't work - what is the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings)==len(data[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8175,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dt\u001b[39m.\u001b[39;49mfit(encodings,data[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    960\u001b[0m         X,\n\u001b[1;32m    961\u001b[0m         y,\n\u001b[1;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    238\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    239\u001b[0m     dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, force_all_finite\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 242\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    243\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m missing_values_in_feature_mask \u001b[39m=\u001b[39m (\n\u001b[1;32m    247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/base.py:617\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[1;32m    616\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 617\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_X_params)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[1;32m    619\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8175,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "dt.fit(encodings,data[\"train\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the problem is that the data is in the wrong shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726605504587156\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "\n",
    "def prepare_text_data(text_list,tokenizer,encoded_length=512):\n",
    "    encodings = []\n",
    "    for curr_example in text_list:\n",
    "        curr_tokens = tokenizer.tokenize(curr_example)\n",
    "        curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "        # truncate sequences that are too long\n",
    "        if len(curr_encodings) > encoded_length:\n",
    "            curr_encodings = curr_encodings[:encoded_length]\n",
    "        # pad sequences that are too short with 0s\n",
    "        elif len(curr_encodings) < encoded_length:\n",
    "            curr_encodings = curr_encodings + [0]*(encoded_length-len(curr_encodings))\n",
    "\n",
    "        encodings.append(curr_encodings)\n",
    "        \n",
    "    return encodings\n",
    "\n",
    "\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Group Discussion\n",
    "\n",
    "Is this a good accuracy score?\n",
    "\n",
    "What context do we need to interpret this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark style=\"background: lightgreen\"> It's okay but could use improvement, if you look at how it is split on the classes it is about 50% so 72% is better than guessing or choosing spam from all predictions</mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306422018348623\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = RandomForestClassifier()\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_labels \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X53sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m lr_model \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X53sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m lr_model\u001b[39m.\u001b[39;49mfit(train_encoding,train_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X53sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m predictions \u001b[39m=\u001b[39m lr_model\u001b[39m.\u001b[39mpredict(test_encoding)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F5_2_MachineLearning.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m( accuracy_score(test_labels,predictions) )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m )(\n\u001b[1;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    462\u001b[0m         t,\n\u001b[1;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    464\u001b[0m         X,\n\u001b[1;32m    465\u001b[0m         y,\n\u001b[1;32m    466\u001b[0m         sample_weight,\n\u001b[1;32m    467\u001b[0m         i,\n\u001b[1;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    960\u001b[0m         X,\n\u001b[1;32m    961\u001b[0m         y,\n\u001b[1;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"ma2za/many_emotions\")\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = RandomForestClassifier()\n",
    "\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Group Exercise\n",
    "\n",
    "Have everyone at your table agree on another classification model that you've used before. \n",
    "\n",
    "Run this code but with the other model. How do the results compare?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Exercise\n",
    "\n",
    "Pick a different *text classification* dataset and test the accuracy of this approach on that data. How do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with this encoding\n",
    "\n",
    "The numerical encoding we've used may not work well in many situations.\n",
    "\n",
    "Think about the predictor features our model is using\n",
    "\n",
    "`['first', 'time', 'poster', 'in', 'this', 'sub',`\n",
    "\n",
    "`[2034, 2051, 13082, 1999, 2023, 4942,`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first feature is \"first token in the sequence\"\n",
    "\n",
    "2nd feature is \"second token in the sequence\"\n",
    "\n",
    "etc.\n",
    "\n",
    "If a word appears in different positions in two different sequences, the model won't inherently recognize that it's the same word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The way words get mapped to numbers is arbitrary - there's no relationship between words that have close numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2034]\n",
      "[2051]\n"
     ]
    }
   ],
   "source": [
    "print( tokenizer.encode(\"first\",add_special_tokens=False) )\n",
    "print( tokenizer.encode(\"time\",add_special_tokens=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-Words Encoding\n",
    "\n",
    "Choose vocabulary (say 5000 most common words) one column for each word\n",
    "\n",
    "row contains counts for each word\n",
    "\n",
    "**Example**\n",
    "\n",
    "*Sentence 1:* \"The cat sat on the hat\"\n",
    "\n",
    "*Sentence 2:* \"The dog ate the cat and the hat\" \n",
    "\n",
    "*Vocabulary:* { the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "\n",
    "|            | the | cat | sat | on | hat | dog | ate | and |\n",
    "|------------|-----|-----|-----|----|-----|-----|-----|-----|\n",
    "| Sentence 1 | 2   | 1   | 1   | 1  | 1   | 0   | 0   | 0   |\n",
    "| Sentence 2 | 3   | 1   | 0   | 0  | 1   | 1   | 1   | 1   |\n",
    "\n",
    "\n",
    "**The downside:** this doesn't maintain any information about word order - thus the \"bag\" of words\n",
    "\n",
    "`scikit-learn` provides a Bag-of-Words encoder called `CountVectorizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "# remove stop words\n",
    "vectorizer = CountVectorizer(max_features=5000,stop_words=\"english\")  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TD-IDF Encoding\n",
    "\n",
    "**TF-IDF:** Term Frequency - Inverse Document Frequency\n",
    "\n",
    "**Term Frequency:** How often does the word appear in the example, like CountVectorizer\n",
    "* actually take the $\\log$ of it\n",
    "\n",
    "**Document Frequency:** What fraction of the *documents* (or *training-examples*) does the word appear in?\n",
    "\n",
    "**Inverse Document Frequency:** (number of documents) / (number of documents containing the word)\n",
    "* if a word is in only a few documents, you get a big number\n",
    "* if a word appears in lots of documents, you get a small number\n",
    "\n",
    "When encoding a new example, multiply the Term Frequency of the word in this example by the Inverse Document Frequency of the training set\n",
    "* gives higher weight to words that are differentiators\n",
    "* stop words should automatically be de-emphasized\n",
    "\n",
    "**Example:**\n",
    "Document collection: all of Shakespeare's plays\n",
    "\n",
    "The word `Romeo` appears 113 times but only in 1 document\n",
    "\n",
    "The word `action` appears 113 time but in 31 documents \n",
    "\n",
    "so Romeo will get a much higher weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Try the Integer Encoding, Count Vectorizer, and TD-IDF on a *new dataset*\n",
    "* use at least three different classification models\n",
    "\n",
    "Give a short write-up on the following\n",
    "* Describe your dataset, including the distribution of the target variable\n",
    "* Describe the results of the machine learning experiment\n",
    "* Interpret the results - why do you think each of the encodings and algorithms performed the way they did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## if time....\n",
    "\n",
    "Here's some code for building a multilayer neural network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 0s 808us/step - loss: 0.2739 - accuracy: 0.9697\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 805us/step - loss: 0.0388 - accuracy: 0.9989\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 796us/step - loss: 0.0156 - accuracy: 0.9991\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 772us/step - loss: 0.0088 - accuracy: 0.9993\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 773us/step - loss: 0.0058 - accuracy: 0.9995\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 771us/step - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 765us/step - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 816us/step - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 830us/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 783us/step - loss: 0.0016 - accuracy: 0.9996\n",
      "86/86 [==============================] - 0s 535us/step - loss: 0.0052 - accuracy: 0.9985\n",
      "Test accuracy: 99.85%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# the sklearn vectors need to be converted to numpy arrays for this library\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays = test_vectors.toarray()\n",
    "# Convert string labels to binary labels\n",
    "train_labels_array = np.array([1 if label == \"spam\" else 0 for label in train_labels])\n",
    "test_labels_array = np.array([1 if label == \"spam\" else 0 for label in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vectors_arrays, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
