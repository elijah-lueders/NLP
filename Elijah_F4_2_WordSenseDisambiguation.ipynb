{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## WordSense Disambiguation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F4_2_WordSenseDisambiguation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Word Senses and WordNet, Chapter 23 of *Speech and Language Processing* by Daniel Jurafsky & James H. Martin: https://web.stanford.edu/~jurafsky/slp3/23.pdf\n",
    "\n",
    "WordNet documentation: https://www.nltk.org/api/nltk.corpus.reader.wordnet.html\n",
    "\n",
    "SemCor Corpus Module documentation: https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n",
    "\n",
    "NLTK Stopwords: https://pythonspot.com/nltk-stop-words/\n",
    "\n",
    "Lemmatization with NLTK: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you shouldn't need to do this in Colab, but I had to do it on my own machine\n",
    "#in order to connect to the nltk service\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Sense Disambiguation\n",
    "\n",
    "As we explored last time, one word can have many *senses*.\n",
    "\n",
    "The **WordNet** database can be used to look up different word senses of a particular word.\n",
    "\n",
    "The task of figuring out which sense is being usede in a given context is called **word sense disambiguation**\n",
    "\n",
    "Important for\n",
    "* extracting proper meaning from text\n",
    "* translation - e.g., different senses of one word in English might have different translations\n",
    "* question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Typical approach for WSD\n",
    "\n",
    "Look at the *context* of a word - what other words are around it\n",
    "\n",
    "For example, consider the word **bank** in \n",
    "\n",
    "\"I need to go to the bank and deposit my paycheck.\"\n",
    "\n",
    "We can determine from *deposit*, *paycheck*, and maybe even *go to* that we're talking about a financial institution and not a river bank. \n",
    "\n",
    "Which definition does the context share the most words with?\n",
    "\n",
    "*Definition 1:* 'sloping land (especially the slope beside a body of water)'\n",
    "\n",
    "*Definition 2:* 'a financial institution that accepts deposits and channels the money into lending activities'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def compute_overlap(set1, set2):\n",
    "    count_overlap = 0\n",
    "    for item in set1:\n",
    "        if item in set2:\n",
    "            count_overlap += 1\n",
    "    return count_overlap\n",
    "\n",
    "\n",
    "sentence = [\"i\", \"need\", \"to\", \"go\", \"to\", \"the\", \"bank\", \"and\", \"deposit\", \"my\", \"paycheck\"]\n",
    "definition1 = [\"sloping\", \"land\", \"especially\", \"the\", \"slope\", \"beside\", \"a\", \"body\", \"of\", \"water\"]\n",
    "definition2 = [\"a\", \"financial\", \"institution\", \"that\", \"accepts\", \"deposits\", \"and\", \"channels\", \"the\", \"money\", \"into\", \"lending\", \"activities\"]\n",
    "\n",
    "print( compute_overlap(sentence,definition1) )\n",
    "print( compute_overlap(sentence,definition2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss: What problems do you see with this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Simplified Lesk Algorithm\n",
    "\n",
    "The **Simplified Lesk Algorithm** loops over all possible word senses to find the one whose definition/examples share the most words in common with the sentence context.\n",
    "\n",
    "Given a `word` and `sentence`\n",
    "1. Make a *set* of all the words in the sentence (my need to tokenize)\n",
    "2. Look up all the `synsets` for `word` in **WordNet**\n",
    "3. Loop through the list of `synsets`\n",
    "    * create a signature - the set of all the words that appear the definition and list of examples for this `word` from **WordNet** (may need to tokenize)\n",
    "    * compute the overlap between the signature and the word context\n",
    "    * if this is better than the previous best overlap, save the new sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss: How should we tokenize our text data for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'need', 'go', 'i', 'and', 'my', 'deposit', 'to', 'paycheck', 'the', 'bank'}\n",
      "Synset('bank.n.01')\n",
      "3\n",
      "Synset('depository_financial_institution.n.01')\n",
      "4\n",
      "Synset('bank.n.03')\n",
      "1\n",
      "Synset('bank.n.04')\n",
      "1\n",
      "Synset('bank.n.05')\n",
      "0\n",
      "Synset('bank.n.06')\n",
      "3\n",
      "Synset('bank.n.07')\n",
      "2\n",
      "Synset('savings_bank.n.02')\n",
      "2\n",
      "Synset('bank.n.09')\n",
      "3\n",
      "Synset('bank.n.10')\n",
      "2\n",
      "Synset('bank.v.01')\n",
      "3\n",
      "Synset('bank.v.02')\n",
      "1\n",
      "Synset('bank.v.03')\n",
      "1\n",
      "Synset('bank.v.04')\n",
      "1\n",
      "Synset('bank.v.05')\n",
      "1\n",
      "Synset('deposit.v.02')\n",
      "2\n",
      "Synset('bank.v.07')\n",
      "3\n",
      "Synset('trust.v.01')\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Synset('trust.v.01')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet') #only need to do this once\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def simplified_lesk(word,sentence):\n",
    "    best_overlap = 0\n",
    "    best_sense = None\n",
    "    \n",
    "    # make a set of the words in the sentence using punkt\n",
    "    \n",
    "    sentence_words = set(word_tokenize(sentence))\n",
    "    print(sentence_words)\n",
    "    \n",
    "    # get the synsets for the word\n",
    "    senses = wn.synsets(word)\n",
    "    # loop through the senses\n",
    "    for sense in senses:\n",
    "        print(sense)\n",
    "        # get the definition\n",
    "        signatures = set(word_tokenize(sense.definition()))\n",
    "        # print(signatures)\n",
    "        for example in sense.examples():\n",
    "            signatures.update(word_tokenize(example.lower()))\n",
    "        # compute the overlap between the sentence and the definition\n",
    "        overlap = compute_overlap(sentence_words,signatures)\n",
    "        print(overlap)\n",
    "        # if the overlap is better than the best overlap so far, update the best sense\n",
    "        if overlap > best_overlap:\n",
    "            best_sense = sense\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "simplified_lesk(\"bank\",\"i need to go to the bank and deposit my paycheck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Exercise: Finish implementing this algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improving the algorithm\n",
    "\n",
    "Two things we could do to try to improve the Lesk algorithm\n",
    "\n",
    "1. Remove tokens that don't carry meaning like punctuation and *stopwords* (words like \"the\", \"is\", \"to\", etc.)\n",
    "\n",
    "2. Lemmatize the words - convert them into their base form\n",
    "\n",
    "Try to catch the word \"deposit(s)\" in \n",
    "* \"a financial institution that accepts **deposits** and channels the money into lending activities'\n",
    "* \"I need to go to the bank and **deposit** my paycheck.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stopwords Corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'such', 'they', 'me', 'where', 'have', \"should've\", 'its', 'then', \"she's\", 'when', \"you're\", 'an', 'or', 'ours', 'ain', 'we', 'between', 'won', 'but', 'did', 'shouldn', 'your', 'he', 'own', 's', 'it', 'theirs', 'does', 'on', 'over', 'her', \"hadn't\", 'haven', 'their', \"isn't\", 'weren', 'aren', \"mightn't\", 'how', \"don't\", 're', \"you've\", 'who', 'further', 'now', 'himself', 'didn', 'with', 'm', 'in', \"wouldn't\", 'here', 'and', \"doesn't\", 'so', 'don', \"wasn't\", 'should', 'those', 'our', 'all', 'not', 'themselves', 'at', 'whom', 've', 'for', 'be', \"hasn't\", 'has', 'any', \"needn't\", 'these', 'hers', 't', \"mustn't\", 'his', \"aren't\", \"didn't\", 'the', \"couldn't\", 'from', 'than', 'up', 'herself', 'ma', 'few', 'very', 'most', 'wasn', 'as', 'will', 'yourselves', 'above', 'too', 'some', 'while', 'by', \"you'd\", 'him', 'after', 'll', 'this', 'was', 'about', 'below', 'myself', 'other', \"shouldn't\", 'just', 'under', 'both', 'what', 'yourself', 'why', 'itself', 'been', 'y', 'against', 'each', 'are', 'needn', \"haven't\", 'isn', 'only', 'doesn', 'she', 'having', 'is', \"you'll\", 'of', 'again', 'same', \"won't\", 'which', 'off', 'can', 'until', 'more', 'to', 'being', 'i', 'mustn', 'because', 'no', 'wouldn', 'am', 'into', 'a', 'hasn', 'during', 'mightn', 'o', 'yours', 'had', 'ourselves', 'shan', \"it's\", 'out', 'that', 'before', \"shan't\", 'once', 'hadn', 'them', 'd', \"that'll\", 'my', 'down', 'through', 'if', 'there', 'nor', \"weren't\", 'you', 'were', 'couldn', 'doing', 'do'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/000794593/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #only need to do this once\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deposit: deposit\n",
      "deposits: deposit\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') #do it once\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"deposit:\", lemmatizer.lemmatize(\"deposit\"))\n",
    "print(\"deposits:\", lemmatizer.lemmatize(\"deposits\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Add stopword removal and lemmatization to your Lesk Algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset for evaluation WSD\n",
    "\n",
    "The SemCor NLTK corpus contains text that has been tagged with WordNet sense (mostly Lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     /Users/elijahlueders/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('semcor') #do this once\n",
    "from nltk.corpus import semcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown1/tagfiles/br-a01.xml', 'brown1/tagfiles/br-a02.xml', 'brown1/tagfiles/br-a11.xml', 'brown1/tagfiles/br-a12.xml', 'brown1/tagfiles/br-a13.xml', 'brown1/tagfiles/br-a14.xml', 'brown1/tagfiles/br-a15.xml', 'brown1/tagfiles/br-b13.xml', 'brown1/tagfiles/br-b20.xml', 'brown1/tagfiles/br-c01.xml', 'brown1/tagfiles/br-c02.xml', 'brown1/tagfiles/br-c04.xml', 'brown1/tagfiles/br-d01.xml', 'brown1/tagfiles/br-d02.xml', 'brown1/tagfiles/br-d03.xml', 'brown1/tagfiles/br-d04.xml', 'brown1/tagfiles/br-e01.xml', 'brown1/tagfiles/br-e02.xml', 'brown1/tagfiles/br-e04.xml', 'brown1/tagfiles/br-e21.xml', 'brown1/tagfiles/br-e24.xml', 'brown1/tagfiles/br-e29.xml', 'brown1/tagfiles/br-f03.xml', 'brown1/tagfiles/br-f10.xml', 'brown1/tagfiles/br-f19.xml', 'brown1/tagfiles/br-f43.xml', 'brown1/tagfiles/br-g01.xml', 'brown1/tagfiles/br-g11.xml', 'brown1/tagfiles/br-g15.xml', 'brown1/tagfiles/br-h01.xml', 'brown1/tagfiles/br-j01.xml', 'brown1/tagfiles/br-j02.xml', 'brown1/tagfiles/br-j03.xml', 'brown1/tagfiles/br-j04.xml', 'brown1/tagfiles/br-j05.xml', 'brown1/tagfiles/br-j06.xml', 'brown1/tagfiles/br-j07.xml', 'brown1/tagfiles/br-j08.xml', 'brown1/tagfiles/br-j09.xml', 'brown1/tagfiles/br-j10.xml', 'brown1/tagfiles/br-j11.xml', 'brown1/tagfiles/br-j12.xml', 'brown1/tagfiles/br-j13.xml', 'brown1/tagfiles/br-j14.xml', 'brown1/tagfiles/br-j15.xml', 'brown1/tagfiles/br-j16.xml', 'brown1/tagfiles/br-j17.xml', 'brown1/tagfiles/br-j18.xml', 'brown1/tagfiles/br-j19.xml', 'brown1/tagfiles/br-j20.xml', 'brown1/tagfiles/br-j22.xml', 'brown1/tagfiles/br-j23.xml', 'brown1/tagfiles/br-j37.xml', 'brown1/tagfiles/br-j52.xml', 'brown1/tagfiles/br-j53.xml', 'brown1/tagfiles/br-j54.xml', 'brown1/tagfiles/br-j55.xml', 'brown1/tagfiles/br-j56.xml', 'brown1/tagfiles/br-j57.xml', 'brown1/tagfiles/br-j58.xml', 'brown1/tagfiles/br-j59.xml', 'brown1/tagfiles/br-j60.xml', 'brown1/tagfiles/br-j70.xml', 'brown1/tagfiles/br-k01.xml', 'brown1/tagfiles/br-k02.xml', 'brown1/tagfiles/br-k03.xml', 'brown1/tagfiles/br-k04.xml', 'brown1/tagfiles/br-k05.xml', 'brown1/tagfiles/br-k06.xml', 'brown1/tagfiles/br-k07.xml', 'brown1/tagfiles/br-k08.xml', 'brown1/tagfiles/br-k09.xml', 'brown1/tagfiles/br-k10.xml', 'brown1/tagfiles/br-k11.xml', 'brown1/tagfiles/br-k12.xml', 'brown1/tagfiles/br-k13.xml', 'brown1/tagfiles/br-k14.xml', 'brown1/tagfiles/br-k15.xml', 'brown1/tagfiles/br-k16.xml', 'brown1/tagfiles/br-k17.xml', 'brown1/tagfiles/br-k18.xml', 'brown1/tagfiles/br-k19.xml', 'brown1/tagfiles/br-k20.xml', 'brown1/tagfiles/br-k21.xml', 'brown1/tagfiles/br-k22.xml', 'brown1/tagfiles/br-k23.xml', 'brown1/tagfiles/br-k24.xml', 'brown1/tagfiles/br-k25.xml', 'brown1/tagfiles/br-k26.xml', 'brown1/tagfiles/br-k27.xml', 'brown1/tagfiles/br-k28.xml', 'brown1/tagfiles/br-k29.xml', 'brown1/tagfiles/br-l11.xml', 'brown1/tagfiles/br-l12.xml', 'brown1/tagfiles/br-m01.xml', 'brown1/tagfiles/br-m02.xml', 'brown1/tagfiles/br-n05.xml', 'brown1/tagfiles/br-p01.xml', 'brown1/tagfiles/br-r05.xml', 'brown1/tagfiles/br-r06.xml', 'brown1/tagfiles/br-r07.xml', 'brown1/tagfiles/br-r08.xml', 'brown1/tagfiles/br-r09.xml', 'brown2/tagfiles/br-e22.xml', 'brown2/tagfiles/br-e23.xml', 'brown2/tagfiles/br-e25.xml', 'brown2/tagfiles/br-e26.xml', 'brown2/tagfiles/br-e27.xml', 'brown2/tagfiles/br-e28.xml', 'brown2/tagfiles/br-e30.xml', 'brown2/tagfiles/br-e31.xml', 'brown2/tagfiles/br-f08.xml', 'brown2/tagfiles/br-f13.xml', 'brown2/tagfiles/br-f14.xml', 'brown2/tagfiles/br-f15.xml', 'brown2/tagfiles/br-f16.xml', 'brown2/tagfiles/br-f17.xml', 'brown2/tagfiles/br-f18.xml', 'brown2/tagfiles/br-f20.xml', 'brown2/tagfiles/br-f21.xml', 'brown2/tagfiles/br-f22.xml', 'brown2/tagfiles/br-f23.xml', 'brown2/tagfiles/br-f24.xml', 'brown2/tagfiles/br-f25.xml', 'brown2/tagfiles/br-f33.xml', 'brown2/tagfiles/br-f44.xml', 'brown2/tagfiles/br-g12.xml', 'brown2/tagfiles/br-g14.xml', 'brown2/tagfiles/br-g16.xml', 'brown2/tagfiles/br-g17.xml', 'brown2/tagfiles/br-g18.xml', 'brown2/tagfiles/br-g19.xml', 'brown2/tagfiles/br-g20.xml', 'brown2/tagfiles/br-g21.xml', 'brown2/tagfiles/br-g22.xml', 'brown2/tagfiles/br-g23.xml', 'brown2/tagfiles/br-g28.xml', 'brown2/tagfiles/br-g31.xml', 'brown2/tagfiles/br-g39.xml', 'brown2/tagfiles/br-g43.xml', 'brown2/tagfiles/br-g44.xml', 'brown2/tagfiles/br-h09.xml', 'brown2/tagfiles/br-h11.xml', 'brown2/tagfiles/br-h12.xml', 'brown2/tagfiles/br-h13.xml', 'brown2/tagfiles/br-h14.xml', 'brown2/tagfiles/br-h15.xml', 'brown2/tagfiles/br-h16.xml', 'brown2/tagfiles/br-h17.xml', 'brown2/tagfiles/br-h18.xml', 'brown2/tagfiles/br-h21.xml', 'brown2/tagfiles/br-h24.xml', 'brown2/tagfiles/br-j29.xml', 'brown2/tagfiles/br-j30.xml', 'brown2/tagfiles/br-j31.xml', 'brown2/tagfiles/br-j32.xml', 'brown2/tagfiles/br-j33.xml', 'brown2/tagfiles/br-j34.xml', 'brown2/tagfiles/br-j35.xml', 'brown2/tagfiles/br-j38.xml', 'brown2/tagfiles/br-j41.xml', 'brown2/tagfiles/br-j42.xml', 'brown2/tagfiles/br-l08.xml', 'brown2/tagfiles/br-l09.xml', 'brown2/tagfiles/br-l10.xml', 'brown2/tagfiles/br-l13.xml', 'brown2/tagfiles/br-l14.xml', 'brown2/tagfiles/br-l15.xml', 'brown2/tagfiles/br-l16.xml', 'brown2/tagfiles/br-l17.xml', 'brown2/tagfiles/br-l18.xml', 'brown2/tagfiles/br-n09.xml', 'brown2/tagfiles/br-n10.xml', 'brown2/tagfiles/br-n11.xml', 'brown2/tagfiles/br-n12.xml', 'brown2/tagfiles/br-n14.xml', 'brown2/tagfiles/br-n15.xml', 'brown2/tagfiles/br-n16.xml', 'brown2/tagfiles/br-n17.xml', 'brown2/tagfiles/br-n20.xml', 'brown2/tagfiles/br-p07.xml', 'brown2/tagfiles/br-p09.xml', 'brown2/tagfiles/br-p10.xml', 'brown2/tagfiles/br-p12.xml', 'brown2/tagfiles/br-p24.xml', 'brown2/tagfiles/br-r04.xml', 'brownv/tagfiles/br-a03.xml', 'brownv/tagfiles/br-a04.xml', 'brownv/tagfiles/br-a05.xml', 'brownv/tagfiles/br-a06.xml', 'brownv/tagfiles/br-a07.xml', 'brownv/tagfiles/br-a08.xml', 'brownv/tagfiles/br-a09.xml', 'brownv/tagfiles/br-a10.xml', 'brownv/tagfiles/br-a16.xml', 'brownv/tagfiles/br-a17.xml', 'brownv/tagfiles/br-a18.xml', 'brownv/tagfiles/br-a19.xml', 'brownv/tagfiles/br-a20.xml', 'brownv/tagfiles/br-a21.xml', 'brownv/tagfiles/br-a22.xml', 'brownv/tagfiles/br-a23.xml', 'brownv/tagfiles/br-a24.xml', 'brownv/tagfiles/br-a25.xml', 'brownv/tagfiles/br-a26.xml', 'brownv/tagfiles/br-a27.xml', 'brownv/tagfiles/br-a28.xml', 'brownv/tagfiles/br-a29.xml', 'brownv/tagfiles/br-a30.xml', 'brownv/tagfiles/br-a31.xml', 'brownv/tagfiles/br-a32.xml', 'brownv/tagfiles/br-a33.xml', 'brownv/tagfiles/br-a34.xml', 'brownv/tagfiles/br-a35.xml', 'brownv/tagfiles/br-a36.xml', 'brownv/tagfiles/br-a37.xml', 'brownv/tagfiles/br-a38.xml', 'brownv/tagfiles/br-a39.xml', 'brownv/tagfiles/br-a40.xml', 'brownv/tagfiles/br-a41.xml', 'brownv/tagfiles/br-a42.xml', 'brownv/tagfiles/br-a43.xml', 'brownv/tagfiles/br-a44.xml', 'brownv/tagfiles/br-b01.xml', 'brownv/tagfiles/br-b02.xml', 'brownv/tagfiles/br-b03.xml', 'brownv/tagfiles/br-b04.xml', 'brownv/tagfiles/br-b05.xml', 'brownv/tagfiles/br-b06.xml', 'brownv/tagfiles/br-b07.xml', 'brownv/tagfiles/br-b08.xml', 'brownv/tagfiles/br-b09.xml', 'brownv/tagfiles/br-b10.xml', 'brownv/tagfiles/br-b11.xml', 'brownv/tagfiles/br-b12.xml', 'brownv/tagfiles/br-b14.xml', 'brownv/tagfiles/br-b15.xml', 'brownv/tagfiles/br-b16.xml', 'brownv/tagfiles/br-b17.xml', 'brownv/tagfiles/br-b18.xml', 'brownv/tagfiles/br-b19.xml', 'brownv/tagfiles/br-b21.xml', 'brownv/tagfiles/br-b22.xml', 'brownv/tagfiles/br-b23.xml', 'brownv/tagfiles/br-b24.xml', 'brownv/tagfiles/br-b25.xml', 'brownv/tagfiles/br-b26.xml', 'brownv/tagfiles/br-b27.xml', 'brownv/tagfiles/br-c03.xml', 'brownv/tagfiles/br-c05.xml', 'brownv/tagfiles/br-c06.xml', 'brownv/tagfiles/br-c07.xml', 'brownv/tagfiles/br-c08.xml', 'brownv/tagfiles/br-c09.xml', 'brownv/tagfiles/br-c10.xml', 'brownv/tagfiles/br-c11.xml', 'brownv/tagfiles/br-c12.xml', 'brownv/tagfiles/br-c13.xml', 'brownv/tagfiles/br-c14.xml', 'brownv/tagfiles/br-c15.xml', 'brownv/tagfiles/br-c16.xml', 'brownv/tagfiles/br-c17.xml', 'brownv/tagfiles/br-d05.xml', 'brownv/tagfiles/br-d06.xml', 'brownv/tagfiles/br-d07.xml', 'brownv/tagfiles/br-d08.xml', 'brownv/tagfiles/br-d09.xml', 'brownv/tagfiles/br-d10.xml', 'brownv/tagfiles/br-d11.xml', 'brownv/tagfiles/br-d12.xml', 'brownv/tagfiles/br-d13.xml', 'brownv/tagfiles/br-d14.xml', 'brownv/tagfiles/br-d15.xml', 'brownv/tagfiles/br-d16.xml', 'brownv/tagfiles/br-d17.xml', 'brownv/tagfiles/br-e03.xml', 'brownv/tagfiles/br-e05.xml', 'brownv/tagfiles/br-e06.xml', 'brownv/tagfiles/br-e07.xml', 'brownv/tagfiles/br-e08.xml', 'brownv/tagfiles/br-e09.xml', 'brownv/tagfiles/br-e10.xml', 'brownv/tagfiles/br-e11.xml', 'brownv/tagfiles/br-e12.xml', 'brownv/tagfiles/br-e13.xml', 'brownv/tagfiles/br-e14.xml', 'brownv/tagfiles/br-e15.xml', 'brownv/tagfiles/br-e16.xml', 'brownv/tagfiles/br-e17.xml', 'brownv/tagfiles/br-e18.xml', 'brownv/tagfiles/br-e19.xml', 'brownv/tagfiles/br-e20.xml', 'brownv/tagfiles/br-f01.xml', 'brownv/tagfiles/br-f02.xml', 'brownv/tagfiles/br-f04.xml', 'brownv/tagfiles/br-f05.xml', 'brownv/tagfiles/br-f06.xml', 'brownv/tagfiles/br-f07.xml', 'brownv/tagfiles/br-f09.xml', 'brownv/tagfiles/br-f11.xml', 'brownv/tagfiles/br-f12.xml', 'brownv/tagfiles/br-g02.xml', 'brownv/tagfiles/br-g03.xml', 'brownv/tagfiles/br-g04.xml', 'brownv/tagfiles/br-g05.xml', 'brownv/tagfiles/br-g06.xml', 'brownv/tagfiles/br-g07.xml', 'brownv/tagfiles/br-g08.xml', 'brownv/tagfiles/br-g09.xml', 'brownv/tagfiles/br-g10.xml', 'brownv/tagfiles/br-g13.xml', 'brownv/tagfiles/br-h02.xml', 'brownv/tagfiles/br-h03.xml', 'brownv/tagfiles/br-h04.xml', 'brownv/tagfiles/br-h05.xml', 'brownv/tagfiles/br-h06.xml', 'brownv/tagfiles/br-h07.xml', 'brownv/tagfiles/br-h08.xml', 'brownv/tagfiles/br-h10.xml', 'brownv/tagfiles/br-j21.xml', 'brownv/tagfiles/br-j24.xml', 'brownv/tagfiles/br-j25.xml', 'brownv/tagfiles/br-j26.xml', 'brownv/tagfiles/br-j27.xml', 'brownv/tagfiles/br-j28.xml', 'brownv/tagfiles/br-l01.xml', 'brownv/tagfiles/br-l02.xml', 'brownv/tagfiles/br-l03.xml', 'brownv/tagfiles/br-l04.xml', 'brownv/tagfiles/br-l05.xml', 'brownv/tagfiles/br-l06.xml', 'brownv/tagfiles/br-l07.xml', 'brownv/tagfiles/br-m03.xml', 'brownv/tagfiles/br-m04.xml', 'brownv/tagfiles/br-m05.xml', 'brownv/tagfiles/br-m06.xml', 'brownv/tagfiles/br-n01.xml', 'brownv/tagfiles/br-n02.xml', 'brownv/tagfiles/br-n03.xml', 'brownv/tagfiles/br-n04.xml', 'brownv/tagfiles/br-n06.xml', 'brownv/tagfiles/br-n07.xml', 'brownv/tagfiles/br-n08.xml', 'brownv/tagfiles/br-p02.xml', 'brownv/tagfiles/br-p03.xml', 'brownv/tagfiles/br-p04.xml', 'brownv/tagfiles/br-p05.xml', 'brownv/tagfiles/br-p06.xml', 'brownv/tagfiles/br-p08.xml', 'brownv/tagfiles/br-r01.xml', 'brownv/tagfiles/br-r02.xml', 'brownv/tagfiles/br-r03.xml']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of file identifiers in SemCor\n",
    "file_ids = semcor.fileids()\n",
    "print(file_ids) #looks like they're from the brown dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Access the sense-tagged sentences from a file\n",
    "sentences = semcor.sents(file_ids[0])\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']], [['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('far.r.02.far'), ['further']), Tree(Lemma('state.v.01.say'), ['said']), ['in'], Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('end.n.02.end'), ['end']), Tree(Lemma('presentment.n.01.presentment'), ['presentments']), ['that'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Executive', 'Committee'])]), [','], ['which'], Tree(Lemma('own.v.01.have'), ['had']), Tree(Lemma('overall.s.02.overall'), ['over-all']), Tree(Lemma('mission.n.03.charge'), ['charge']), ['of'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['``'], Tree(Lemma('deserve.v.01.deserve'), ['deserves']), ['the'], Tree(Lemma('praise.n.01.praise'), ['praise']), ['and'], Tree(Lemma('thanks.n.01.thanks'), ['thanks']), ['of'], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['City', 'of', 'Atlanta'])]), [\"''\"], ['for'], ['the'], Tree(Lemma('manner.n.01.manner'), ['manner']), ['in'], ['which'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), ['was'], Tree(Lemma('conduct.v.01.conduct'), ['conducted']), ['.']], ...]\n"
     ]
    }
   ],
   "source": [
    "# Access the sense tags for those sentences\n",
    "tags = semcor.tagged_sents(file_ids[0],tag=\"sem\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a complex format - notice that some (but not all!) of the words are grouped together in a tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The']\n",
      "(Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
      "(Lemma('state.v.01.say') said)\n",
      "(Lemma('friday.n.01.Friday') Friday)\n",
      "['an']\n",
      "(Lemma('probe.n.01.investigation') investigation)\n",
      "['of']\n",
      "(Lemma('atlanta.n.01.Atlanta') Atlanta)\n",
      "[\"'s\"]\n",
      "(Lemma('late.s.03.recent') recent)\n",
      "(Lemma('primary.n.01.primary_election') primary election)\n",
      "(Lemma('produce.v.04.produce') produced)\n",
      "['``']\n",
      "['no']\n",
      "(Lemma('evidence.n.01.evidence') evidence)\n",
      "[\"''\"]\n",
      "['that']\n",
      "['any']\n",
      "(Lemma('abnormality.n.04.irregularity') irregularities)\n",
      "(Lemma('happen.v.01.take_place') took place)\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "# tags[0] is the tags for the first sentence, sentence[0]\n",
    "for tag in tags[0]:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice \n",
    "* Some tokens don't have a tag - stopwords, punctuation, etc. - these show up as a string inside a list\n",
    "* \"Fulton County Grand Jury\" is grouped under Lemma('group.n.01.group')\n",
    "* \"primary election\" is grouped as a compound word with Lemma('primary.n.01.primary_election')\n",
    "\n",
    "This is going to be tough to work with. Here's an attempt to loop through them, match them up wit the word from the sentence, and handle these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The\n",
      "Tag: ['The']\n",
      "\n",
      "Word: Fulton\n",
      "Tag: (Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
      "Words in this group: ['Fulton', 'County', 'Grand', 'Jury']\n",
      "\n",
      "Word: said\n",
      "Tag: (Lemma('state.v.01.say') said)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m  \u001b[39mtype\u001b[39m(tag\u001b[39m.\u001b[39mlabel()) \u001b[39m!=\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     actual_sense \u001b[39m=\u001b[39m tag\u001b[39m.\u001b[39mlabel()\u001b[39m.\u001b[39msynset()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     pred_sense \u001b[39m=\u001b[39m simplified_lesk(word,sentences[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m#this is where you could check if you correctly matched the actual sense\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m word_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m#advance to next word\u001b[39;00m\n",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m best_sense \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# make a set of the words in the sentence using punkt\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sentence_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(word_tokenize(sentence))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(sentence_words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F4_2_WordSenseDisambiguation.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# get the synsets for the word\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text):\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[39m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[39m=\u001b[39m text[previous_slice\u001b[39m.\u001b[39mstop : match\u001b[39m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# for keeping track of which word and tag we're on\n",
    "word_idx = 0\n",
    "tag_idx = 0\n",
    "    \n",
    "while tag_idx < len(tags[0]) and word_idx < len(sentences[0]):\n",
    "    word = sentences[0][word_idx] #the current word\n",
    "    tag = tags[0][tag_idx] #the tag for the current word\n",
    "    \n",
    "    # check for tags that got assigned to compound words like primary_election\n",
    "    if len(tag) > 1:\n",
    "        print(\"Word:\",sentences[0][word_idx:(word_idx+len(tag)-1)])\n",
    "        print(\"Tag:\",tag)\n",
    "        word_idx += len(tag) #move to the next word that isn't part of the compound\n",
    "        \n",
    "    # for Tree objects, check if it really tagged a word and not a group\n",
    "    elif type(tag) is nltk.Tree and type(tag[0]) is str:\n",
    "        print(\"Word:\",word)\n",
    "        print(\"Tag:\",tag)\n",
    "\n",
    "        # here's how we can get the synset for tags that give us a Lemma\n",
    "        if  type(tag.label()) != str:\n",
    "            actual_sense = tag.label().synset()\n",
    "            pred_sense = simplified_lesk(word,sentences[0])\n",
    "            #this is where you could check if you correctly matched the actual sense\n",
    "            \n",
    "        word_idx += 1 #advance to next word\n",
    "        \n",
    "    # check if it's a punctuation/stopword - if we got here, it means tag was not of type nltk.Tree    \n",
    "    elif type(tag[0]) is str:\n",
    "        print(\"Word:\",word)\n",
    "        print(\"Tag:\",tag)\n",
    "        word_idx += 1\n",
    "        \n",
    "    # If we get gerem it means the Tree contained a group of words, and we can count\n",
    "    # how many with len( tag.leaves() )\n",
    "    else:\n",
    "        print(\"Word:\",word)\n",
    "        print(\"Tag:\",tag)\n",
    "        print(\"Words in this group:\",tag.leaves())\n",
    "        word_idx += len(tag.leaves())\n",
    "    tag_idx += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "For cases where the SemCor dataset has a single word tagged with a WordNet sense, run your `simplified_lesk` code on it and see if it matches. Go through all of the sentences in a particular file_id and compute an accuracy score.\n",
    "\n",
    "Write notes here on what you did and the results you got."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
