{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Loading Data and Evaluating Classification Models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F1_2_DataEvaluation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "Hugging Face *Load a dataset from the Hub tutorial*: https://huggingface.co/docs/datasets/load_hub\n",
    "\n",
    "scikit-learn *Classification Metrics User's Guide*: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installing the datasets module\n",
    "\n",
    "Hugging Face *also* provides a lot of data sets, and there's a module for that.\n",
    "\n",
    "If you're running in a new environment (like Colab), you'll also have to install transformers again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.10/site-packages (4.32.1)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.10/site-packages (2.14.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elijahlueders/Library/Python/3.10/lib/python/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Python/3.10/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/elijahlueders/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's load the go_emotions dataset\n",
    "\n",
    "The *go_emotions* dataset is a set of Reddit comments that have been labeled with 28 categories (27 emotions + neutral). \n",
    "\n",
    "See more here: https://huggingface.co/datasets/go_emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's explore the dataset\n",
    "\n",
    "What does it look like when printed/displayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Hugging Face has defined a special datatype for representing a dataset - `DatasetDict`\n",
    "\n",
    "They've divided it into three parts:\n",
    "* **train:** these should be used to train models\n",
    "* **validation:** these should be used for testing model in the middle of training (for picking the right metaparameters, etc.)\n",
    "* **test:** these should be used to evaluate the model\n",
    "\n",
    "**Review Question:** Why is it important to train and test models on different data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Now let's access the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'id'],\n",
      "    num_rows: 5427\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from there, we can use subscript notation to access the text, labels, and ids\n",
    "\n",
    "What are the types of these things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print( type(dataset[\"test\"][\"text\"]) )\n",
    "print( type(dataset[\"test\"][\"labels\"]) )\n",
    "print( type(dataset[\"test\"][\"id\"]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're just basic Python lists - let's print the first few to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 texts:\n",
      "['I’m really sorry about your situation :( Although I love the names Sapphira, Cirilla, and Scarlett!', \"It's wonderful because it's awful. At not with.\", 'Kings fan here, good luck to you guys! Will be an interesting game to watch! ', \"I didn't know that, thank you for teaching me something today!\", 'They got bored from haunting earth for thousands of years and ultimately moved on to the afterlife.']\n",
      "\n",
      "The first 5 labels:\n",
      "[[25], [0], [13], [15], [27]]\n",
      "\n",
      "The first 5 ids:\n",
      "['eecwqtt', 'ed5f85d', 'een27c3', 'eelgwd1', 'eem5uti']\n"
     ]
    }
   ],
   "source": [
    "print(\"The first 5 texts:\")\n",
    "print( dataset[\"test\"][\"text\"][0:5] )\n",
    "\n",
    "print(\"\\nThe first 5 labels:\")\n",
    "print( dataset[\"test\"][\"labels\"][0:5] )\n",
    "\n",
    "print(\"\\nThe first 5 ids:\")\n",
    "print( dataset[\"test\"][\"id\"][0:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "They also made it possible to select/slice on the Dataset object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But which emotions are represented by those numbers 25, 0, 13, 15, and 27?\n",
    "\n",
    "A `dataset` has a `features` attribute which stores this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you can get the string for an individual feature like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"].features[\"labels\"].feature.int2str(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the dataset with a model\n",
    "\n",
    "So, let's put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "classifier = pipeline(task=\"text-classification\", tokenizer=tokenizer, model=model)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"][0:5])\n",
    "print(\"Here are the predictions\")\n",
    "print(results)\n",
    "\n",
    "total_correct = 0\n",
    "\n",
    "#comparing to the actual labels\n",
    "for idx in range (5):\n",
    "    print(\"\\nText:\",dataset[\"test\"][\"text\"][idx])\n",
    "    predicted_label = results[idx][\"label\"]\n",
    "    actual_label_numeric = dataset[\"test\"][\"labels\"][idx][0]\n",
    "    actual_label = dataset[\"test\"].features[\"labels\"].feature.int2str( actual_label_numeric )\n",
    "    print(\"Predicted label:\",predicted_label,\", Actual label:\",actual_label)\n",
    "    if predicted_label == actual_label:\n",
    "        total_correct += 1\n",
    "        \n",
    "print(\"Accuracy:\",(total_correct/5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test it out:** Do this for a larger portion of the test set (it might take a while)\n",
    "- This will go faster if you can run on a GPU/TPU\n",
    "- In Colab, do Runtime -> Change runtime type (The pipeline should automatically try to use a GPU if it is available)\n",
    "- Comment out the print statements for individual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making it work with the scikit-learn metrics\n",
    "\n",
    "**Review:** The `scikit-learn` library has lots of functions for calculating metrics of machine-learning predictions. \n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
    "\n",
    "To get this to work with our experiment, just make lists for your predicted and actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"][0:1000])\n",
    "\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "for idx in range(1000):\n",
    "    predicted_labels.append(results[idx][\"label\"])\n",
    "    actual_label_numeric = dataset[\"test\"][\"labels\"][idx][0]\n",
    "    actual_labels.append( dataset[\"test\"].features[\"labels\"].feature.int2str( actual_label_numeric ) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### accuracy\n",
    "\n",
    "Now we can calculate accuracy using `sklearn`'s `accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(actual_labels,predicted_labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty good for 28 categories\n",
    "\n",
    "if you were just guessing randomly, you'd get 1/28 ~= 0.036\n",
    "\n",
    "if you always guess the most common label (neutral), you'd get ~ 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### precision, recall, f1 scores\n",
    "\n",
    "You can display all of these with the classification report - higher numbers are better.\n",
    "\n",
    "**precision:** Of all the instances that the model predicted as **admiration**, how many were actually **admiration**?\n",
    "\n",
    "**recall:** Of all the actual **admiration** instances in the dataset, how many did the model correctly predict as **admiration**?\n",
    "\n",
    "**f1:** Combination balancing *precision* and *recall*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(actual_labels,predicted_labels,zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to perform well on emotions like amusement and gratitude, but not as well on realization or annoyance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### confusion matrix\n",
    "\n",
    "display a confusion matrix to see which labels are getting confused with others\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_names = dataset[\"test\"].features[\"labels\"].feature.names\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cm = confusion_matrix(actual_labels,predicted_labels,labels=label_names)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=label_names)\n",
    "cmd.plot(ax=ax,xticks_rotation='vertical',)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neutral is the most common, so it gets confused with others most\n",
    "\n",
    "also notice sometimes others get confused: love and approval, optimism and desire, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Group work\n",
    "\n",
    "Choose **two** of the new *text classification* models your group experimented with last week that you can find the datasets for (https://huggingface.co/datasets if not linked directly).\n",
    "\n",
    "Split into two subgroups\n",
    "* each subgroup: evaluate one of the models using the metrics shown here\n",
    "\n",
    "**Prepare to debrief:** I will have you present at least one set of results per group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running inference on other datasets\n",
    "\n",
    "Let's do this again, but use the default positive/negative sentiment analysis model and see how it does with the `go_emotions` data.\n",
    "\n",
    "We do need to tell the confusion matrix about the labels from the classifier too: `[\"POSITIVE\",\"NEGATIVE\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"][0:1000])\n",
    "\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "for idx in range(1000):\n",
    "    predicted_labels.append(results[idx][\"label\"])\n",
    "    actual_label_numeric = dataset[\"test\"][\"labels\"][idx][0]\n",
    "    actual_labels.append( dataset[\"test\"].features[\"labels\"].feature.int2str( actual_label_numeric ) )\n",
    "    \n",
    "\n",
    "label_names = [\"POSITIVE\",\"NEGATIVE\"]+dataset[\"test\"].features[\"labels\"].feature.names\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cm = confusion_matrix(actual_labels,predicted_labels,labels=label_names)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=label_names)\n",
    "cmd.plot(ax=ax,xticks_rotation='vertical',)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emotions like anger, annoyance, and disapproval often result in a NEGATIVE prediction\n",
    "\n",
    "admiration, gratitude, and love often result in a POSITIVE prediction\n",
    "\n",
    "You could manually sort the emotions into POSITIVE and NEGATIVE and run your own experiment to measure how well it does on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Go to the Hugging Face models page: https://huggingface.co/models\n",
    "* Click `Text Classification`\n",
    "* Find a different model and a dataset appropriate for testing it with than the ones we worked with today\n",
    "    - many models will link to the datasets they were trained on, but you can find others at https://huggingface.co/datasets\n",
    "    - write down some info about the models you found\n",
    "        - what is it for?\n",
    "        - who made it?\n",
    "        - what kind of data was it trained on?\n",
    "        - are they based on some other model and trained on new data (*fine-tuned*) for a specific task?\n",
    "    - write down some info on the dataset you found\n",
    "        - where did it come from?\n",
    "        - how big is it?\n",
    "        - what kind of labels does it classify?\n",
    "* Evaluate the performance \n",
    "    - use some of the metrics we talked about today\n",
    "    - describe in your own words how it performed\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
