{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "  <div>\n",
    "      <h1>CS 195: Natural Language Processing</h1>\n",
    "      <h2>Neural Language Modeling</h2>\n",
    "      </br>\n",
    "    <a href=\"https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_1_NeuralLanguageModeling.ipynb\">\n",
    "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
    "    </a>\n",
    "  </div>\n",
    "  <div style=\"margin-left: 20px;\">\n",
    "    <img src=\"images/dalle_neural_net_viz.png\" width=\"500\" style=\"display: block;\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Cover Illustration:** generated by Dall E using the ChatGPT 4 interface, prompted for a visualization of the network used in the code below. *That's not what I meant.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcement\n",
    "\n",
    "AI - English Faculty Candidate: Gabriel Ford\n",
    "\n",
    "Meeting with students: Thursday at 3:30pm in Howard 309\n",
    "\n",
    "Scholarly Presentation: Friday at 9:00am in Howard ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "SLP: Neural Networks and Neural Language Models, Chapter 7 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: keras in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: tensorflow in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: packaging in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset for today\n",
    "\n",
    "AG News dataset\n",
    "* short news articles\n",
    "* four classes: World, Sports, Business, Sci/Tech\n",
    "\n",
    "https://huggingface.co/datasets/ag_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "print(data[\"train\"][\"text\"][0])\n",
    "\n",
    "# 0 is World\n",
    "# 1 is Sports\n",
    "# 2 is Business\n",
    "# 3 is Sci/Tech\n",
    "print(data[\"train\"][\"label\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Text Classification with Keras\n",
    "\n",
    "Last time, we saw \n",
    "* how to do text classification when there are more than 2 classes\n",
    "    - one hot encoded output layer, one node per class, *softmax* output\n",
    "    - categorical crossentropy loss\n",
    "* embedding layer\n",
    "    - pad sequences to all be same length\n",
    "    - learn vector for each word representing word semantics\n",
    "    \n",
    "<div>\n",
    "    <img src=\"images/neural_text_classification.png\">\n",
    "</div>\n",
    "\n",
    "image source: SLP Fig. 7.11, https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "\n",
    "*pooling* combines/aggregates all of the embeddings in some way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 39s 10ms/step - loss: 0.3229 - accuracy: 0.8802\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 41s 11ms/step - loss: 0.1127 - accuracy: 0.9628\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 39s 10ms/step - loss: 0.0313 - accuracy: 0.9912\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 37s 10ms/step - loss: 0.0166 - accuracy: 0.9961\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 36s 10ms/step - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 36s 10ms/step - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 36s 10ms/step - loss: 0.0056 - accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 37s 10ms/step - loss: 0.0044 - accuracy: 0.9984\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 36s 10ms/step - loss: 0.0036 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 39s 10ms/step - loss: 0.0032 - accuracy: 0.9987\n",
      "238/238 [==============================] - 0s 470us/step - loss: 0.6719 - accuracy: 0.8945\n",
      "Test accuracy: 89.45%\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "# Prepare the tokenizer and fit on the training text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"train\"][\"text\"])\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(data[\"train\"][\"text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(data[\"test\"][\"text\"])\n",
    "\n",
    "# Pad sequences to ensure uniform length; you can decide the max length based on your dataset's characteristics\n",
    "max_length = 100  # This should be adjusted based on the dataset\n",
    "train_encoding_array = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_encoding_array = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "train_labels_array = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_array = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "#create a neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "#use one of these instead of Flatten() to try a pooling method\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(20, input_dim=max_length, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_encoding_array, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_encoding_array, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Language Modeling\n",
    "\n",
    "**Neural Language Modeling:** predict next word(s) from previous ones - like what we did with Markov models\n",
    "\n",
    "Like classification, but output is softmax of every possible word in the vocabulary\n",
    "\n",
    "Often a first step before extending the model to do summarization, translation, dialog, and other NLP tasks\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/neural_language_modeling.png\">\n",
    "</div>\n",
    "\n",
    "image source: SLP Fig. 7.13, https://web.stanford.edu/~jurafsky/slp3/7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Neural Language Model in Keras\n",
    "\n",
    "Let's start by sampling some data from our news dataset\n",
    "\n",
    "Then split into a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7867\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "data_subset, _ = train_test_split(data[\"train\"][\"text\"],train_size=1000)\n",
    "train_data, test_data = train_test_split(data_subset,train_size=0.8)\n",
    "\n",
    "# Prepare the tokenizer and fit on the training text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_subset)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size:\",vocabulary_size)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "train_texts = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing training examples\n",
    "\n",
    "We want to take the sequences like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1004, 200, 749, 50, 4145, 749, 50, 1197, 1468, 24, 2574, 4, 1004, 4, 3, 153, 2, 642, 216, 1861, 2, 4146, 182, 9, 10, 1862, 374, 8, 266, 1469]\n"
     ]
    }
   ],
   "source": [
    "print( train_texts[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and slide a window across to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use [1004, 200, 749, 50, 4145] to predict 749\n",
      "Use [200, 749, 50, 4145, 749] to predict 50\n",
      "Use [749, 50, 4145, 749, 50] to predict 1197\n",
      "Use [50, 4145, 749, 50, 1197] to predict 1468\n",
      "etc.\n"
     ]
    }
   ],
   "source": [
    "print(\"Use\",train_texts[0][0:5],\"to predict\",train_texts[0][5])\n",
    "print(\"Use\",train_texts[0][1:6],\"to predict\",train_texts[0][6])\n",
    "print(\"Use\",train_texts[0][2:7],\"to predict\",train_texts[0][7])\n",
    "print(\"Use\",train_texts[0][3:8],\"to predict\",train_texts[0][8])\n",
    "print(\"etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Group Discussion\n",
    "\n",
    "What data structures (lists, matrixes, etc.) do we need to prepare to make this a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing all of the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 27669\n",
      "First train text: [1004, 200, 749, 50, 4145, 749, 50, 1197, 1468, 24, 2574, 4, 1004, 4, 3, 153, 2, 642, 216, 1861, 2, 4146, 182, 9, 10, 1862, 374, 8, 266, 1469]\n",
      "Example sequence 0: [1004, 200, 749, 50, 4145]  target: 749\n",
      "Example sequence 1: [200, 749, 50, 4145, 749]  target: 50\n",
      "Example sequence 2: [749, 50, 4145, 749, 50]  target: 1197\n",
      "Example sequence 3: [50, 4145, 749, 50, 1197]  target: 1468\n",
      "Example sequence 4: [4145, 749, 50, 1197, 1468]  target: 24\n",
      "Example sequence 5: [749, 50, 1197, 1468, 24]  target: 2574\n"
     ]
    }
   ],
   "source": [
    "# Decide the sequence length\n",
    "sequence_length = 5  # Length of the input sequence before predicting the next word\n",
    "\n",
    "# Create the sequences\n",
    "predictor_sequences = []\n",
    "targets = []\n",
    "for text in train_texts:\n",
    "    for i in range(sequence_length, len(text)):\n",
    "        # Take the sequence of tokens as input and the next token as target\n",
    "        curr_target = text[i]\n",
    "        curr_predictor_sequence = text[i-sequence_length:i]\n",
    "        predictor_sequences.append(curr_predictor_sequence)\n",
    "        targets.append(curr_target)\n",
    "\n",
    "        \n",
    "print(\"Number of sequences:\",len(predictor_sequences))\n",
    "\n",
    "\n",
    "print(\"First train text:\",train_texts[0])\n",
    "print(\"Example sequence 0:\",predictor_sequences[0],\" target:\",targets[0])\n",
    "print(\"Example sequence 1:\",predictor_sequences[1],\" target:\",targets[1])\n",
    "print(\"Example sequence 2:\",predictor_sequences[2],\" target:\",targets[2])\n",
    "print(\"Example sequence 3:\",predictor_sequences[3],\" target:\",targets[3])\n",
    "print(\"Example sequence 4:\",predictor_sequences[4],\" target:\",targets[4])\n",
    "print(\"Example sequence 5:\",predictor_sequences[5],\" target:\",targets[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding\n",
    "\n",
    "Some of the sequences might be really short - so we'll pad them just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform length\n",
    "predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The target output\n",
    "\n",
    "Since we're making this into a classification problem, the output layer needs to have one node for each word in the vocabulary. \n",
    "\n",
    "Target values need to be transformed into a one-hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors words 0: [1004  200  749   50 4145]\n",
      "target word 0: 749\n",
      "target word 0 one hot encoded: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert output to one-hot encoding\n",
    "target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
    "\n",
    "print(\"Predictors words 0:\", predictor_sequences_padded[0])\n",
    "print(\"target word 0:\", targets[0])\n",
    "print(\"target word 0 one hot encoded:\", target_word_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing the test set\n",
    "\n",
    "We have to do all of those same things for the test set.\n",
    "\n",
    "**Group Exercise:** Turn this into a function so that you can use it to prepare both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 143  143 3731  445 6880]\n",
      " [ 143 3731  445 6880  484]\n",
      " [3731  445 6880  484   25]\n",
      " ...\n",
      " [  33  281   32  894    7]\n",
      " [ 281   32  894    7    3]\n",
      " [  32  894    7    3  607]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_texts = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Create the sequences\n",
    "predictor_sequences_test = []\n",
    "targets_test = []\n",
    "for text in test_texts:\n",
    "    for i in range(sequence_length, len(text)):\n",
    "        # Take the sequence of tokens as input and the next token as target\n",
    "        curr_target = text[i]\n",
    "        curr_predictor_sequence = text[i-sequence_length:i]\n",
    "        predictor_sequences_test.append(curr_predictor_sequence)\n",
    "        targets_test.append(curr_target)\n",
    "        \n",
    "# Pad sequences to ensure uniform length\n",
    "predictor_sequences_padded_test = pad_sequences(predictor_sequences_test, maxlen=sequence_length, padding='pre')\n",
    "\n",
    "# Convert target to one-hot encoding\n",
    "target_word_one_hot_test = to_categorical(targets_test, num_classes=vocabulary_size)\n",
    "\n",
    "print(predictor_sequences_padded_test)\n",
    "print(target_word_one_hot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(data, seq_len, tokenizer):\n",
    "    texts = tokenizer.texts_to_sequences(data)\n",
    "    vocabulary_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Create the sequences\n",
    "    predictor_sequences = []\n",
    "    targets = []\n",
    "    for text in texts:\n",
    "        for i in range(seq_len, len(text)):\n",
    "            # Take the sequence of tokens as input and the next token as target\n",
    "            curr_target = text[i]\n",
    "            curr_predictor_sequence = text[i-seq_len:i]\n",
    "            predictor_sequences.append(curr_predictor_sequence)\n",
    "            targets.append(curr_target)\n",
    "            \n",
    "    # Pad sequences to ensure uniform length\n",
    "    predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=seq_len, padding='pre')\n",
    "\n",
    "    # Convert target to one-hot encoding\n",
    "    target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
    "\n",
    "    return predictor_sequences_padded, target_word_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 143  143 3731  445 6880]\n",
      " [ 143 3731  445 6880  484]\n",
      " [3731  445 6880  484   25]\n",
      " ...\n",
      " [  33  281   32  894    7]\n",
      " [ 281   32  894    7    3]\n",
      " [  32  894    7    3  607]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predictor_sequences_padded_test, target_word_one_hot_test = prepare_data(test_data, sequence_length, tokenizer)\n",
    "print(predictor_sequences_padded_test)\n",
    "print(target_word_one_hot_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Designing the Neural Network\n",
    "\n",
    "We'll start with a simple network like the one we used for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "865/865 [==============================] - 5s 5ms/step - loss: 7.7091 - accuracy: 0.0455 - val_loss: 7.5106 - val_accuracy: 0.0490\n",
      "Epoch 2/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 6.9437 - accuracy: 0.0631 - val_loss: 7.5948 - val_accuracy: 0.0731\n",
      "Epoch 3/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 6.4816 - accuracy: 0.0858 - val_loss: 7.8779 - val_accuracy: 0.0849\n",
      "Epoch 4/10\n",
      "865/865 [==============================] - 5s 5ms/step - loss: 6.0830 - accuracy: 0.1072 - val_loss: 8.1970 - val_accuracy: 0.0897\n",
      "Epoch 5/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 5.6764 - accuracy: 0.1341 - val_loss: 8.5932 - val_accuracy: 0.0937\n",
      "Epoch 6/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 5.2100 - accuracy: 0.1787 - val_loss: 9.1442 - val_accuracy: 0.0927\n",
      "Epoch 7/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 4.6721 - accuracy: 0.2335 - val_loss: 9.8598 - val_accuracy: 0.0865\n",
      "Epoch 8/10\n",
      "865/865 [==============================] - 5s 5ms/step - loss: 4.0475 - accuracy: 0.2990 - val_loss: 10.8593 - val_accuracy: 0.0835\n",
      "Epoch 9/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 3.3317 - accuracy: 0.3974 - val_loss: 12.2157 - val_accuracy: 0.0773\n",
      "Epoch 10/10\n",
      "865/865 [==============================] - 4s 5ms/step - loss: 2.5423 - accuracy: 0.5232 - val_loss: 13.9154 - val_accuracy: 0.0747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14fc38a00>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model - you can also pass in the test set\n",
    "model.fit(predictor_sequences_padded, target_word_one_hot, epochs=10, verbose=1, validation_data=(predictor_sequences_padded_test, target_word_one_hot_test))\n",
    "\n",
    "# The model can now be used to predict the next word in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing the final model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 0s 2ms/step - loss: 13.9154 - accuracy: 0.0747\n",
      "Test accuracy: 7.47%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(predictor_sequences_padded_test, target_word_one_hot_test)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "We can use this model to successively generate words based on previous ones - like our Markov sequence on steroids.\n",
    "\n",
    "Let's see how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 91, 20, 12, 19]]\n",
      "[[ 1 91 20 12 19]]\n"
     ]
    }
   ],
   "source": [
    "starter_string = \"the government said that it\"\n",
    "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
    "print(tokens_list)\n",
    "\n",
    "tokens_array = np.array(tokens_list)\n",
    "print(tokens_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model will predict probabilities for each possible word in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4457838e-24 7.3149940e-04 2.6042967e-06 ... 2.2408966e-17\n",
      "  2.0150673e-16 3.8289548e-27]]\n",
      "We get a probability for each of the 7867 words\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = model.predict(tokens_array,verbose=0)\n",
    "print(predicted_probabilities)\n",
    "print(\"We get a probability for each of the\",len(predicted_probabilities[0]),\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we could get the word associated with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: 18\n",
      "word: is\n"
     ]
    }
   ],
   "source": [
    "predicted_index = np.argmax(predicted_probabilities)\n",
    "print(\"word index:\",predicted_index)\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "print(\"word:\",predicted_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you could generate a random word according the these probabilities (like with did with Markov text generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is selling its tyco global network to help 39 s positive test day 5 million on a quarter earlier old players announced monday that it is selling its tyco global network to help 39 s positive test day 5 million on a quarter earlier old players announced monday that it "
     ]
    }
   ],
   "source": [
    "starter_string = \"the government said that it\"\n",
    "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
    "tokens = tokens_list[0]\n",
    "\n",
    "for i in range(50):\n",
    "    curr_seq = tokens[-sequence_length:]\n",
    "    curr_array = np.array([curr_seq])\n",
    "    predicted_probabilities = model.predict(curr_array,verbose=0)\n",
    "    predicted_index = np.argmax(predicted_probabilities)\n",
    "    predicted_word = tokenizer.index_word[predicted_index]\n",
    "    print(predicted_word+\" \",end=\"\")\n",
    "    tokens.append(predicted_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Pick another dataset and get it working with this code\n",
    "* you will likely need to prepare the text a little differently - do you need to first break it into sentences?\n",
    "* describe your dataset and what you did to prepare it\n",
    "\n",
    "Perform a neural language modeling experiment\n",
    "* experiment with something to try to find a well-performing model\n",
    "    * sliding window size\n",
    "    * number of hidden nodes in the network\n",
    "    * learning rate\n",
    "* describe what you did and write up an interpretation of your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 53s 42ms/step - loss: 0.5760 - accuracy: 0.6862 - val_loss: 0.4885 - val_accuracy: 0.7774\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 0.4416 - accuracy: 0.7987 - val_loss: 0.4544 - val_accuracy: 0.7938\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 0.3977 - accuracy: 0.8267 - val_loss: 0.4549 - val_accuracy: 0.7980\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 0.3751 - accuracy: 0.8363 - val_loss: 0.4625 - val_accuracy: 0.8004\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 53s 43ms/step - loss: 0.3597 - accuracy: 0.8454 - val_loss: 0.4631 - val_accuracy: 0.7968\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4637 - accuracy: 0.7904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01\n",
      "Test Accuracy: 0.7904000282287598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 0.3194 - accuracy: 0.8668 - val_loss: 0.4692 - val_accuracy: 0.8034\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 47s 38ms/step - loss: 0.2801 - accuracy: 0.8876 - val_loss: 0.4801 - val_accuracy: 0.8042\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 48s 39ms/step - loss: 0.2513 - accuracy: 0.9018 - val_loss: 0.4926 - val_accuracy: 0.8012\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.2280 - accuracy: 0.9124 - val_loss: 0.5114 - val_accuracy: 0.8042\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 48s 39ms/step - loss: 0.2083 - accuracy: 0.9214 - val_loss: 0.5291 - val_accuracy: 0.7992\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5309 - accuracy: 0.7964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n",
      "Test Accuracy: 0.7964000105857849\n",
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 51s 40ms/step - loss: 0.1845 - accuracy: 0.9333 - val_loss: 0.5323 - val_accuracy: 0.7994\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 0.1798 - accuracy: 0.9356 - val_loss: 0.5370 - val_accuracy: 0.7996\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.1761 - accuracy: 0.9373 - val_loss: 0.5419 - val_accuracy: 0.8002\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.1730 - accuracy: 0.9387 - val_loss: 0.5456 - val_accuracy: 0.7994\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.1700 - accuracy: 0.9398 - val_loss: 0.5492 - val_accuracy: 0.7986\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5469 - accuracy: 0.7992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001\n",
      "Test Accuracy: 0.7991999983787537\n",
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 0.1665 - accuracy: 0.9414 - val_loss: 0.5497 - val_accuracy: 0.7986\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 0.1662 - accuracy: 0.9414 - val_loss: 0.5501 - val_accuracy: 0.7988\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.1659 - accuracy: 0.9416 - val_loss: 0.5504 - val_accuracy: 0.7988\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.1657 - accuracy: 0.9418 - val_loss: 0.5508 - val_accuracy: 0.7990\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 48s 39ms/step - loss: 0.1654 - accuracy: 0.9420 - val_loss: 0.5512 - val_accuracy: 0.7990\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5486 - accuracy: 0.7976\n",
      "Learning Rate: 1e-05\n",
      "Test Accuracy: 0.7975999712944031\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "# load the dataset\n",
    "vocab_size = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "x = np.concatenate((x_train, x_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_sequences(x)\n",
    "\n",
    "max_sequence_length = 100  # You can set a different maximum sequence length\n",
    "x_padded = pad_sequences(x, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_padded, y, train_size = 0.8, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# different learning rates\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]  # Add or remove learning rates as needed\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile the model with the current learning rate\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'Learning Rate: {learning_rate}')\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #007acc; color: white; border-radius: 10px; padding: 10px; width: fit-content; max-width: 70%; font-size: 1rem;\">\n",
    "    a learning rate around 0.001 appears to strike a balance between training stability and model performance for this sentiment analysis task on the IMDb dataset. Further tuning with other parameters could give better results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "#an example on changing the learning rate\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
