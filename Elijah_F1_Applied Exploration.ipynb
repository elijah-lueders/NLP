{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fortnight 1: Applied Exploratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demoed for: Conrad, Saul, Katja, Caitlyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "### *pulled from F1_2*\n",
    "\n",
    "Go to the Hugging Face models page: https://huggingface.co/models\n",
    "* Click `Text Classification`\n",
    "* Find a different model and a dataset appropriate for testing it with than the ones we worked with today\n",
    "    - many models will link to the datasets they were trained on, but you can find others at https://huggingface.co/datasets\n",
    "    - write down some info about the models you found\n",
    "        - what is it for?\n",
    "        - who made it?\n",
    "        - what kind of data was it trained on?\n",
    "        - are they based on some other model and trained on new data (*fine-tuned*) for a specific task?\n",
    "    - write down some info on the dataset you found\n",
    "        - where did it come from?\n",
    "        - how big is it?\n",
    "        - what kind of labels does it classify?\n",
    "* Evaluate the performance \n",
    "    - use some of the metrics we talked about today\n",
    "    - describe in your own words how it performed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: filelock in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.33.2\n",
      "Uninstalling transformers-4.33.2:\n",
      "  Would remove:\n",
      "    /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/bin/transformers-cli\n",
      "    /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers-4.33.2.dist-info/*\n",
      "    /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip uninstall transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_reviews_multi\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "        num_rows: 200000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"test\"][\"review_id\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': Value(dtype='string', id=None),\n",
       " 'product_id': Value(dtype='string', id=None),\n",
       " 'reviewer_id': Value(dtype='string', id=None),\n",
       " 'stars': Value(dtype='int32', id=None),\n",
       " 'review_body': Value(dtype='string', id=None),\n",
       " 'review_title': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'product_category': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# loop through features and print each type\n",
    "for feature_name in dataset[\"test\"].features:\n",
    "    print(type(dataset[\"test\"][feature_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': ['en_0199937',\n",
       "  'en_0863335',\n",
       "  'en_0565010',\n",
       "  'en_0963290',\n",
       "  'en_0238156'],\n",
       " 'product_id': ['product_en_0902516',\n",
       "  'product_en_0348072',\n",
       "  'product_en_0356154',\n",
       "  'product_en_0583322',\n",
       "  'product_en_0487636'],\n",
       " 'reviewer_id': ['reviewer_en_0097389',\n",
       "  'reviewer_en_0601537',\n",
       "  'reviewer_en_0970351',\n",
       "  'reviewer_en_0216125',\n",
       "  'reviewer_en_0514203'],\n",
       " 'stars': [1, 1, 1, 1, 1],\n",
       " 'review_body': ['These are AWFUL. They are see through, the fabric feels like tablecloth, and they fit like children’s clothing. Customer service did seem to be nice though, but I regret missing my return date for these. I wouldn’t even donate them because the quality is so poor.',\n",
       "  'I bought 4 and NONE of them worked. Yes I used new batteries!',\n",
       "  \"On first use it didn't heat up and now it doesn't work at all\",\n",
       "  \"You want an HONEST answer? I just returned from UPS where I returned the FARCE of an earring set to Amazon. It did NOT look like what I saw on Amazon. Only a baby would be able to wear the size of the earring. They were SO small. the size of a pin head I at first thought Amazon had forgotten to enclose them in the bag! I didn't bother to take them out of the bag and you can have them back. Will NEVER order another thing from your company. A disgrace. Honest enough for you? Grandma\",\n",
       "  \"The glue works fine but the container is impossible to work with. The cap doesn't come off without plyers and then won't go back on without a violent abrupt force involving both hands and a solid object (desk drawer). This happened even though I was careful to not gum up the lid or tapering snout.\"],\n",
       " 'review_title': ['Don’t waste your time!',\n",
       "  'One Star',\n",
       "  'Totally useless',\n",
       "  'Gold filled earrings',\n",
       "  'Poor container'],\n",
       " 'language': ['en', 'en', 'en', 'en', 'en'],\n",
       " 'product_category': ['apparel',\n",
       "  'other',\n",
       "  'other',\n",
       "  'jewelry',\n",
       "  'industrial_supplies']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': Value(dtype='string', id=None),\n",
       " 'product_id': Value(dtype='string', id=None),\n",
       " 'reviewer_id': Value(dtype='string', id=None),\n",
       " 'stars': Value(dtype='int32', id=None),\n",
       " 'review_body': Value(dtype='string', id=None),\n",
       " 'review_title': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'product_category': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"].features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 10 predictions\n",
      "-------\n",
      "Sample 1 of 10:\n",
      "These are AWFUL. They are see through, the fabric feels like tablecloth, and they fit like children’s clothing. Customer service did seem to be nice though, but I regret missing my return date for these. I wouldn’t even donate them because the quality is so poor.\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 2 of 10:\n",
      "I bought 4 and NONE of them worked. Yes I used new batteries!\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: positive\n",
      "-------\n",
      "Sample 3 of 10:\n",
      "On first use it didn't heat up and now it doesn't work at all\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 4 of 10:\n",
      "You want an HONEST answer? I just returned from UPS where I returned the FARCE of an earring set to Amazon. It did NOT look like what I saw on Amazon. Only a baby would be able to wear the size of the earring. They were SO small. the size of a pin head I at first thought Amazon had forgotten to enclose them in the bag! I didn't bother to take them out of the bag and you can have them back. Will NEVER order another thing from your company. A disgrace. Honest enough for you? Grandma\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 5 of 10:\n",
      "The glue works fine but the container is impossible to work with. The cap doesn't come off without plyers and then won't go back on without a violent abrupt force involving both hands and a solid object (desk drawer). This happened even though I was careful to not gum up the lid or tapering snout.\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 6 of 10:\n",
      "I was over the moon when I got this. I love sunflowers and it looked exactly like they advertised, unfortunately I wore this piercing for one weekend. By Sunday night the sunflower had came off. I will not be purchasing from this company again. Very disappointed.\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 7 of 10:\n",
      "Gotta say, not impressed with the quality. I mean, I can't say I expected it to be overwhelmingly amazing with the cost being where it's at. But, certainly I thought it would last longer than a few days. Had the phone in my gym bag. Slid out from the side and barely tapped the wall. On the aluminum side. Not even on the glass. And low and behold, instantaneous crack. Oddly enough, just on the back. Not a splinter at all on the front. Not sure if the back is intentionally more susceptible to damage or just my luck. Either way, I have to give this product a thumbs down. Wish it held up better. It really is a nice design. I am a fan of how the product looks and the concept. No issue there. Just not durable at all.\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 8 of 10:\n",
      "I would give this zero stars if I could. This is 6.3 you ounces of low grade chocolate for $60. My 9 year old daughter was so disappointed. It didn't even have a different piece for each day. It was mostly flat square wrapped milk chocolate with no design at all!!! Very very poor quality and expensive!\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 9 of 10:\n",
      "Two of the glasses were broken when I opened the package. Could you please be careful for packaging glass items.\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Sample 10 of 10:\n",
      "Doesn’t even work . Did nothing for me :(\n",
      "Stars: 1\n",
      "Actual label: negative\n",
      "Predicted label: negative\n",
      "-------\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/distilbert-base-multilingual-cased-sentiment-2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"philschmid/distilbert-base-multilingual-cased-sentiment-2\")\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"Here are the {len(results)} predictions\")\n",
    "# print(results)\n",
    "\n",
    "total_correct = 0\n",
    "for idx in range(sample_size):\n",
    "    print(f\"-------\\nSample {idx+1} of {sample_size}:\")\n",
    "    print(dataset[\"test\"][\"review_body\"][idx])\n",
    "\n",
    "\n",
    "    actual_label_numeric = dataset[\"test\"][\"stars\"][idx]\n",
    "    print(f\"Stars: {actual_label_numeric}\")\n",
    "\n",
    "    if actual_label_numeric == 3:\n",
    "        actual_label = \"neutral\"\n",
    "    elif actual_label_numeric > 3:\n",
    "        actual_label = \"positive\"\n",
    "    else:\n",
    "        actual_label = \"negative\"\n",
    "    print(f\"Actual label: {actual_label}\")\n",
    "\n",
    "    predicted_label = results[idx][\"label\"]\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "    if predicted_label == actual_label:\n",
    "        total_correct += 1\n",
    "\n",
    "print(f\"-------\\nAccuracy: {total_correct / sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1000 samples, the accuracy is 0.929\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "sample_size = 1000\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/distilbert-base-multilingual-cased-sentiment-2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"philschmid/distilbert-base-multilingual-cased-sentiment-2\")\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "\n",
    "total_correct = 0\n",
    "for idx in range(sample_size):\n",
    "\n",
    "\n",
    "    actual_label_numeric = dataset[\"test\"][\"stars\"][idx]\n",
    "\n",
    "    if actual_label_numeric == 3:\n",
    "        actual_label = \"neutral\"\n",
    "    elif actual_label_numeric > 3:\n",
    "        actual_label = \"positive\"\n",
    "    else:\n",
    "        actual_label = \"negative\"\n",
    "\n",
    "    predicted_label = results[idx][\"label\"]\n",
    "\n",
    "    if predicted_label == actual_label:\n",
    "        total_correct += 1\n",
    "\n",
    "print(f\"For {len(results)} samples, the accuracy is {total_correct / sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(results, dataset):\n",
    "    total_correct = 0\n",
    "    sample_size = len(results)\n",
    "\n",
    "    for idx in range(sample_size):\n",
    "        actual_label_numeric = dataset[\"test\"][\"stars\"][idx]\n",
    "\n",
    "        if actual_label_numeric == 3:\n",
    "            actual_label = \"neutral\"\n",
    "        elif actual_label_numeric > 3:\n",
    "            actual_label = \"positive\"\n",
    "        else:\n",
    "            actual_label = \"negative\"\n",
    "\n",
    "        predicted_label = results[idx][\"label\"]\n",
    "\n",
    "        if predicted_label == actual_label:\n",
    "            total_correct += 1\n",
    "\n",
    "    return total_correct / sample_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(results, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1500 samples, the accuracy is 0.862\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1500\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2000 samples, the accuracy is 0.837\n"
     ]
    }
   ],
   "source": [
    "sample_size = 2000\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2500 samples, the accuracy is 0.74\n"
     ]
    }
   ],
   "source": [
    "sample_size = 2500\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2500 samples, the accuracy is 0.7464\n"
     ]
    }
   ],
   "source": [
    "sample_size = 2500\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1000 samples, the accuracy is 0.737\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 500 samples, the accuracy is 0.766\n"
     ]
    }
   ],
   "source": [
    "sample_size = 500\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1000 samples, the accuracy is 0.74\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "sample_size = 1000\n",
    "results = classifier(dataset[\"test\"][\"review_body\"][0:sample_size])\n",
    "print(f\"For {sample_size} samples, the accuracy is {calculate_accuracy(results, dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (582) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F1_Demo.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F1_Demo.ipynb#Y135sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m classifier(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mreview_body\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m:\u001b[39m3500\u001b[39;49m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:789\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    787\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 789\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    790\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    791\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    792\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    793\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    794\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    795\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    796\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    797\u001b[0m )\n\u001b[1;32m    798\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    799\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:607\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    605\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 607\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    610\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[1;32m    611\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    616\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:135\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    131\u001b[0m     position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand_as(input_ids)  \u001b[39m# (bs, max_seq_length)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m embeddings \u001b[39m=\u001b[39m input_embeds \u001b[39m+\u001b[39;49m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (582) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "results = classifier(dataset[\"test\"][\"review_body\"][0:3500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
