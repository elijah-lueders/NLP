{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Conversational Models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_3_ConversationalModels.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Reference\n",
    "\n",
    "Hugging Face documentation on Blenderbot small: https://huggingface.co/docs/transformers/model_doc/blenderbot-small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder: Applied Exploration\n",
    "\n",
    "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
    "\n",
    "See the [workshop from last time](https://github.com/ericmanley/F23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)\n",
    "\n",
    "Fine-tune an existing model with the following requirements\n",
    "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2. \n",
    "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
    "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1.\n",
    "\n",
    "The Hugging Face NLP course has [examples of fine-tuning for many different tasks](https://huggingface.co/learn/nlp-course/chapter7/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.14.7)\n",
      "Requirement already satisfied: keras in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: tensorflow in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: packaging in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before we get started: Attention Visualizations\n",
    "\n",
    "These are all from the **Attention is all you Need** paper here: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "This shows how much attention the word `making` gave to other words in the sequence. Different heads are shown in different hues\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "        <img src=\"images/attention_vis1.png\">\n",
    "    </center>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three different heads for the same sentence\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td><img src=\"images/attention_vis2a.png\" width=350></td>\n",
    "                <td><img src=\"images/attention_vis2b.png\" width=350></td>\n",
    "                <td><img src=\"images/attention_vis2c.png\" width=350></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conversational Models\n",
    "\n",
    "Models used by chat bots are similar to other sequence-to-sequence models (summarization, translation, question answering), but they have been trained on transcripts of dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading up a Conversational Model\n",
    "\n",
    "Blenderbot Small is a small variation that should be relatively fast to fine tune.\n",
    "\n",
    "You can find other variants on the Hugging Face models repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBlenderbotSmallForConditionalGeneration.\n",
      "\n",
      "Some layers of TFBlenderbotSmallForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['final_logits_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_name = \"facebook/blenderbot_small-90M\"\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating the first input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My friends are cool but they eat too many carbs.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "UTTERANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Tokenizing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
       "array([[  42,  643,   46, 1430,   45,   52, 1176,  146,  177,  753, 2430,\n",
       "           5]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([UTTERANCE], return_tensors=\"tf\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Generating the model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n",
       "array([[   1,   44,  444,   10,  753, 2430,   59,   52, 1176,   20,   14,\n",
       "          67,    8,   30,   70,  165,   72,  753, 2430,    5,    2,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
    "reply_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what kind of carbs do they eat? i don't know much about carbs.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_reply = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "decoded_reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued turns in the conversation\n",
    "\n",
    "For dialogue, you need to pass the model the entire chat history\n",
    "\n",
    "This model separates the chat messages with special `__start__` and `__end__` tokens to help the model figure out the flow of conversation.\n",
    "\n",
    "Other models might use different separators like `<sep>` or just `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My friends are cool but they eat too many carbs.__end____start__what kind of carbs do they eat? i don't know much about carbs__end__ __start__I'm not sure\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLY = \"I'm not sure\"\n",
    "\n",
    "NEXT_UTTERANCE = \"My friends are cool but they eat too many carbs.__end__\" \n",
    "NEXT_UTTERANCE += \"__start__what kind of carbs do they eat? i don't know much about carbs__end__ \"\n",
    "NEXT_UTTERANCE += \"__start__\"+REPLY\n",
    "\n",
    "NEXT_UTTERANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they eat a lot of carbs. carbs are high in protein, fats, and fats.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"tf\")\n",
    "next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
    "tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Write a loop that repeats this automatically. Prompt the user, add the user's input onto the conversation, get the model's reply, add it to the conversation, and so on.\n",
    "\n",
    "Make sure that each time you generate a new response, you pass in the inputs for the entire conversation (the tokenizer should truncate it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\">>> \")\n",
    "next_utterance = \"__start__\" + user_input + \"__end__\"\n",
    "#loop to keep conversation going\n",
    "while user_input != \"quit\":\n",
    "    next_utterance = \"__start__\" + decoded_reply + \"__end__\"\n",
    "    next_utterance += \"__start__\" + input(\">>> \") + \"__end__\"\n",
    "    inputs = tokenizer([next_utterance], return_tensors=\"tf\")\n",
    "    next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
    "    decoded_reply = tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]\n",
    "    print(decoded_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training for Conversation\n",
    "\n",
    "To train for conversation, you need data that consists of user inputs and responses.\n",
    "\n",
    "This code is essentially the same as our original Fine-Tuning code, but we'll use it with a conversational model `\"facebook/blenderbot_small-90M\"` and a dataset consisting of ChatGPT transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBlenderbotSmallForConditionalGeneration.\n",
      "\n",
      "Some layers of TFBlenderbotSmallForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['final_logits_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_name = \"facebook/blenderbot_small-90M\"\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "# I'm using the test split because it is much smaller\n",
    "dataset = load_dataset(\"Open-Orca/SlimOrca\",split=\"train\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Select a small sample\n",
    "sample_size = 50  # Define your sample size\n",
    "sample_dataset = shuffled_dataset.select(range(sample_size))\n",
    "\n",
    "#if you want to use the entire dataset just uncomment the following\n",
    "#sample_dataset = shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
       "   'weight': None},\n",
       "  {'from': 'human',\n",
       "   'value': 'Alan B. Miller Hall, location, Virginia; Alan B. Miller Hall, owner, College of William & Mary; Mason School of Business, country, United States; Alan B. Miller Hall, currentTenants, Mason School of Business\\n\\nWhat is sentence that verbalizes this data?',\n",
       "   'weight': 0.0},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Alan B. Miller Hall is a building located in Virginia, United States, and is owned by the College of William & Mary. The Mason School of Business is currently the main tenant of the hall, and they are also part of the same college in the United States.',\n",
       "   'weight': 1.0}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying an example conversation\n",
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "The preprocessing step is the biggest difference\n",
    "\n",
    "In this example, I'm choosing to concatenate the system and human prompts with the GPT output as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for curr_conv in example['conversations']:\n",
    "    \n",
    "        prompt = \"\"\n",
    "\n",
    "        for idx in range(len(curr_conv)-1):\n",
    "            prompt += curr_conv[idx][\"from\"] + \" \"  #should be either \"system\" or \"human\" - theoretically could be an earlier \"gpt\" if there is more than one gpt response\n",
    "            prompt += curr_conv[idx][\"value\"] + \" \" #associated prompt\n",
    "\n",
    "        response = curr_conv[-1][\"value\"] #should be the gpt response  \n",
    "        \n",
    "        input_texts.append(prompt)\n",
    "        target_texts.append(response)\n",
    "\n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(target_texts, max_length=512, truncation=True, padding='max_length')\n",
    "    #move the target tokens into the model_inputs as the \"decoder_input_ids\"\n",
    "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's what one example looks like preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[423, 15, 46, 12, 10078, 2023, 6, 73, 300, 1492, 5644, 5, 124, 71, 15, 46, 8070, 11, 12, 323, 169, 217, 5, 650, 3546, 354, 5, 3732, 775, 6, 1664, 6, 25176, 318, 337, 118, 3546, 354, 5, 3732, 775, 6, 2380, 6, 422, 10, 894, 553, 694, 332, 118, 5464, 153, 10, 455, 6, 544, 6, 247, 9326, 987, 118, 3546, 354, 5, 3732, 775, 6, 21111, 1602, 12479, 6, 5464, 153, 10, 455, 4, 44, 24, 4720, 22, 1196, 372, 27848, 36, 1419, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoder_input_ids': [[3546, 354, 5, 3732, 775, 24, 12, 440, 499, 13, 2067, 6, 247, 271, 6, 9, 24, 1515, 35, 7, 422, 10, 894, 553, 1310, 5, 7, 5464, 153, 10, 455, 24, 1262, 7, 550, 17013, 10, 7, 775, 6, 9, 52, 46, 98, 191, 10, 7, 209, 422, 13, 7, 247, 271, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[3546, 354, 5, 3732, 775, 24, 12, 440, 499, 13, 2067, 6, 247, 271, 6, 9, 24, 1515, 35, 7, 422, 10, 894, 553, 1310, 5, 7, 5464, 153, 10, 455, 24, 1262, 7, 550, 17013, 10, 7, 775, 6, 9, 52, 46, 98, 191, 10, 7, 209, 422, 13, 7, 247, 271, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(sample_dataset[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use `map` to apply it to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8aa2ee4611f4b6ebdff25bcbbd36556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = sample_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_no_text = tokenized_dataset.remove_columns([\"conversations\"])\n",
    "tokenized_dataset_no_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_dataset_no_text,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the optimizer in the same way as before\n",
    "\n",
    "The main difference here is that this model needed the SparseCategoricalCrossentropy loss function defined explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_epochs = 8\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5.6e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "6/6 [==============================] - 144s 23s/step - loss: 11.9934\n",
      "Epoch 2/8\n",
      "6/6 [==============================] - 131s 22s/step - loss: 8.9231\n",
      "Epoch 3/8\n",
      "6/6 [==============================] - 127s 22s/step - loss: 7.5075\n",
      "Epoch 4/8\n",
      "6/6 [==============================] - 129s 22s/step - loss: 6.1890\n",
      "Epoch 5/8\n",
      "6/6 [==============================] - 129s 21s/step - loss: 5.0500\n",
      "Epoch 6/8\n",
      "6/6 [==============================] - 126s 21s/step - loss: 4.1938\n",
      "Epoch 7/8\n",
      "6/6 [==============================] - 135s 23s/step - loss: 3.5423\n",
      "Epoch 8/8\n",
      "6/6 [==============================] - 136s 23s/step - loss: 3.2218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f71973a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the creator of the universe... of all the things in the universe, of all things, of everything.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahlueders/git/nlp_local/Elijah_F7_3_ConversationalModels.ipynb Cell 39\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F7_3_ConversationalModels.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m next_utterance \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__start__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m user_input \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__end__\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F7_3_ConversationalModels.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer([next_utterance], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F7_3_ConversationalModels.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m next_reply_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],attention_mask\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F7_3_ConversationalModels.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m decoded_reply \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(next_reply_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahlueders/git/nlp_local/Elijah_F7_3_ConversationalModels.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m>>> \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/generation/tf_utils.py:980\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m    972\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    973\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    979\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m    981\u001b[0m         input_ids,\n\u001b[1;32m    982\u001b[0m         max_length\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    983\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m    984\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    985\u001b[0m         length_penalty\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mlength_penalty,\n\u001b[1;32m    986\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mearly_stopping,\n\u001b[1;32m    987\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m    988\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m    989\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m    990\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    991\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    994\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m    995\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_beams \u001b[39m<\u001b[39m generation_config\u001b[39m.\u001b[39mnum_return_sequences:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/generation/tf_utils.py:2589\u001b[0m, in \u001b[0;36mTFGenerationMixin.beam_search\u001b[0;34m(self, input_ids, do_sample, max_length, pad_token_id, eos_token_id, length_penalty, early_stopping, logits_processor, logits_warper, num_return_sequences, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m \u001b[39m# 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m \u001b[39m# NOT yield EOS token though)\u001b[39;00m\n\u001b[1;32m   2578\u001b[0m maximum_iterations \u001b[39m=\u001b[39m max_length \u001b[39m-\u001b[39m cur_len\n\u001b[1;32m   2579\u001b[0m (\n\u001b[1;32m   2580\u001b[0m     cur_len,\n\u001b[1;32m   2581\u001b[0m     running_sequences,\n\u001b[1;32m   2582\u001b[0m     running_scores,\n\u001b[1;32m   2583\u001b[0m     running_beam_indices,\n\u001b[1;32m   2584\u001b[0m     sequences,\n\u001b[1;32m   2585\u001b[0m     scores,\n\u001b[1;32m   2586\u001b[0m     beam_indices,\n\u001b[1;32m   2587\u001b[0m     is_sent_finished,\n\u001b[1;32m   2588\u001b[0m     _,\n\u001b[0;32m-> 2589\u001b[0m ) \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhile_loop(\n\u001b[1;32m   2590\u001b[0m     beam_search_cond_fn,\n\u001b[1;32m   2591\u001b[0m     beam_search_body_fn,\n\u001b[1;32m   2592\u001b[0m     (\n\u001b[1;32m   2593\u001b[0m         cur_len,\n\u001b[1;32m   2594\u001b[0m         running_sequences,\n\u001b[1;32m   2595\u001b[0m         running_scores,\n\u001b[1;32m   2596\u001b[0m         running_beam_indices,\n\u001b[1;32m   2597\u001b[0m         sequences,\n\u001b[1;32m   2598\u001b[0m         scores,\n\u001b[1;32m   2599\u001b[0m         beam_indices,\n\u001b[1;32m   2600\u001b[0m         is_sent_finished,\n\u001b[1;32m   2601\u001b[0m         model_kwargs,\n\u001b[1;32m   2602\u001b[0m     ),\n\u001b[1;32m   2603\u001b[0m     maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[1;32m   2604\u001b[0m )\n\u001b[1;32m   2606\u001b[0m \u001b[39m# 6. prepare outputs\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m \u001b[39m# Account for the edge-case where there are no finished sequences for a particular batch item. If so, return\u001b[39;00m\n\u001b[1;32m   2608\u001b[0m \u001b[39m# running sequences for that batch item.\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m none_finished \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_any(is_sent_finished, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:648\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    641\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    642\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    643\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mwill be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m\n\u001b[1;32m    647\u001b[0m             (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date), instructions)\n\u001b[0;32m--> 648\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:252\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwhile_loop\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m     47\u001b[0m \u001b[39m@deprecation\u001b[39m\u001b[39m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     48\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m                   maximum_iterations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m                   name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m   \u001b[39mreturn\u001b[39;00m while_loop(\n\u001b[1;32m    253\u001b[0m       cond\u001b[39m=\u001b[39;49mcond,\n\u001b[1;32m    254\u001b[0m       body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    255\u001b[0m       loop_vars\u001b[39m=\u001b[39;49mloop_vars,\n\u001b[1;32m    256\u001b[0m       shape_invariants\u001b[39m=\u001b[39;49mshape_invariants,\n\u001b[1;32m    257\u001b[0m       parallel_iterations\u001b[39m=\u001b[39;49mparallel_iterations,\n\u001b[1;32m    258\u001b[0m       back_prop\u001b[39m=\u001b[39;49mback_prop,\n\u001b[1;32m    259\u001b[0m       swap_memory\u001b[39m=\u001b[39;49mswap_memory,\n\u001b[1;32m    260\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    261\u001b[0m       maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[1;32m    262\u001b[0m       return_same_structure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:499\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    496\u001b[0m loop_var_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(type_spec\u001b[39m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    497\u001b[0m                                         \u001b[39mlist\u001b[39m(loop_vars))\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m cond(\u001b[39m*\u001b[39mloop_vars):\n\u001b[0;32m--> 499\u001b[0m   loop_vars \u001b[39m=\u001b[39m body(\u001b[39m*\u001b[39;49mloop_vars)\n\u001b[1;32m    500\u001b[0m   \u001b[39mif\u001b[39;00m try_to_pack \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loop_vars, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    501\u001b[0m     packed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:490\u001b[0m, in \u001b[0;36mwhile_loop.<locals>.<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m    487\u001b[0m     loop_vars \u001b[39m=\u001b[39m (counter, loop_vars)\n\u001b[1;32m    488\u001b[0m     cond \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (  \u001b[39m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[1;32m    489\u001b[0m         math_ops\u001b[39m.\u001b[39mlogical_and(i \u001b[39m<\u001b[39m maximum_iterations, orig_cond(\u001b[39m*\u001b[39mlv)))\n\u001b[0;32m--> 490\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, orig_body(\u001b[39m*\u001b[39;49mlv))\n\u001b[1;32m    491\u001b[0m   try_to_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m executing_eagerly:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/generation/tf_utils.py:2375\u001b[0m, in \u001b[0;36mTFGenerationMixin.beam_search.<locals>.beam_search_body_fn\u001b[0;34m(cur_len, running_sequences, running_scores, running_beam_indices, sequences, scores, beam_indices, is_sent_finished, model_kwargs)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[39m# 2. Compute log probs\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[39m# get log probabilities from logits, process logits with processors (*e.g.* min_length, ...), and\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \u001b[39m# add new logprobs to existing running logprobs scores.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m log_probs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mlog_softmax(logits)\n\u001b[0;32m-> 2375\u001b[0m log_probs \u001b[39m=\u001b[39m logits_processor(flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs), cur_len)\n\u001b[1;32m   2376\u001b[0m log_probs \u001b[39m=\u001b[39m unflatten_beam_dim(log_probs, num_beams)\n\u001b[1;32m   2377\u001b[0m \u001b[39mif\u001b[39;00m do_sample:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/generation/tf_logits_process.py:94\u001b[0m, in \u001b[0;36mTFLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, cur_len, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, cur_len, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     93\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, cur_len)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/generation/tf_logits_process.py:440\u001b[0m, in \u001b[0;36mTFNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores, cur_len)\u001b[0m\n\u001b[1;32m    437\u001b[0m banned_tokens_indices_mask \u001b[39m=\u001b[39m []\n\u001b[1;32m    438\u001b[0m \u001b[39mfor\u001b[39;00m banned_tokens_slice \u001b[39min\u001b[39;00m banned_tokens:\n\u001b[1;32m    439\u001b[0m     banned_tokens_indices_mask\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 440\u001b[0m         [\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m banned_tokens_slice \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vocab_size)]\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m scores \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mwhere(tf\u001b[39m.\u001b[39mconvert_to_tensor(banned_tokens_indices_mask, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mbool), \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m), scores)\n\u001b[1;32m    445\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages/transformers/generation/tf_logits_process.py:440\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m banned_tokens_indices_mask \u001b[39m=\u001b[39m []\n\u001b[1;32m    438\u001b[0m \u001b[39mfor\u001b[39;00m banned_tokens_slice \u001b[39min\u001b[39;00m banned_tokens:\n\u001b[1;32m    439\u001b[0m     banned_tokens_indices_mask\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 440\u001b[0m         [\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m banned_tokens_slice \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vocab_size)]\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m scores \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mwhere(tf\u001b[39m.\u001b[39mconvert_to_tensor(banned_tokens_indices_mask, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mbool), \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m), scores)\n\u001b[1;32m    445\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "user_input = input(\">>> \")\n",
    "next_utterance = \"\"\n",
    "#loop to keep conversation going\n",
    "while user_input != \"quit\":\n",
    "    \n",
    "    next_utterance += \"__start__\" + user_input + \"__end__\"\n",
    "    inputs = tokenizer([next_utterance], return_tensors=\"tf\")\n",
    "    next_reply_ids = model.generate(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"])\n",
    "    decoded_reply = tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0]\n",
    "    user_input = input(\">>> \")\n",
    "    next_utterance += \"__start__\" + decoded_reply + \"__end__\"\n",
    "    user_input = input(\">>> \")\n",
    "    print(decoded_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
