{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Handling Long-Term Information in Recurrent Neural Networks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_4_LongTermRecurrence.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "SLP: RNNs and LSTMs, Chapter 9 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "Wikipedia article on Gated Recurrent Unit: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "\n",
    "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras by Jason Brownlee: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mdatasets\u001b[39m\n",
      "  • \u001b[36mkeras\u001b[39m\n",
      "  • \u001b[36mtransformers\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Using version \u001b[39;1m^2.15.0\u001b[39;22m for \u001b[36mtensorflow\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(1.0s)\u001b[39;22m\n",
      "\n",
      "\u001b[31;1mThe current project's Python requirement (>=3.10,<4.0) is not compatible with some of the required packages Python requirement:\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.12, so it will not be satisfied for Python >=3.12,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.11, so it will not be satisfied for Python >=3.11,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.11, so it will not be satisfied for Python >=3.11,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.12, so it will not be satisfied for Python >=3.12,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.12, so it will not be satisfied for Python >=3.12,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.12, so it will not be satisfied for Python >=3.12,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.12, so it will not be satisfied for Python >=3.12,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.12, so it will not be satisfied for Python >=3.12,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.11, so it will not be satisfied for Python >=3.11,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.11, so it will not be satisfied for Python >=3.11,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.11, so it will not be satisfied for Python >=3.11,<4.0\n",
      "  - tensorflow-io-gcs-filesystem requires Python >=3.7, <3.11, so it will not be satisfied for Python >=3.11,<4.0\n",
      "\n",
      "Because no versions of tensorflow-io-gcs-filesystem match >0.23.1,<0.24.0 || >0.24.0,<0.25.0 || >0.25.0,<0.26.0 || >0.26.0,<0.27.0 || >0.27.0,<0.28.0 || >0.28.0,<0.29.0 || >0.29.0,<0.30.0 || >0.30.0,<0.31.0 || >0.31.0,<0.32.0 || >0.32.0,<0.33.0 || >0.33.0,<0.34.0 || >0.34.0\n",
      " and tensorflow-io-gcs-filesystem (0.23.1) requires Python >=3.7, <3.11, tensorflow-io-gcs-filesystem is forbidden.\n",
      "And because tensorflow-io-gcs-filesystem (0.24.0) requires Python >=3.7, <3.11, tensorflow-io-gcs-filesystem is forbidden.\n",
      "And because tensorflow-io-gcs-filesystem (0.33.0) requires Python >=3.7, <3.12\n",
      " and tensorflow-io-gcs-filesystem (0.32.0) requires Python >=3.7, <3.12, tensorflow-io-gcs-filesystem is forbidden.\n",
      "And because tensorflow-io-gcs-filesystem (0.31.0) requires Python >=3.7, <3.12\n",
      " and tensorflow-io-gcs-filesystem (0.30.0) requires Python >=3.7, <3.12, tensorflow-io-gcs-filesystem is forbidden.\n",
      "And because tensorflow-io-gcs-filesystem (0.29.0) requires Python >=3.7, <3.12\n",
      " and tensorflow-io-gcs-filesystem (0.28.0) requires Python >=3.7, <3.11, tensorflow-io-gcs-filesystem is forbidden.\n",
      "And because tensorflow-io-gcs-filesystem (0.27.0) requires Python >=3.7, <3.11\n",
      " and tensorflow-io-gcs-filesystem (0.26.0) requires Python >=3.7, <3.11, tensorflow-io-gcs-filesystem is forbidden.\n",
      "And because tensorflow-io-gcs-filesystem (0.25.0) requires Python >=3.7, <3.11\n",
      " and tensorflow-io-gcs-filesystem (0.34.0) requires Python >=3.7, <3.12, tensorflow-io-gcs-filesystem is forbidden.\n",
      "Because no versions of tensorflow match >2.15.0,<3.0.0\n",
      " and tensorflow (2.15.0) depends on tensorflow-io-gcs-filesystem (>=0.23.1), tensorflow (>=2.15.0,<3.0.0) requires tensorflow-io-gcs-filesystem (>=0.23.1).\n",
      "Thus, tensorflow is forbidden.\n",
      "So, because nlp-local depends on tensorflow (^2.15.0), version solving failed.\u001b[39;22m\n",
      "\n",
      "  \u001b[34;1m• \u001b[39;22m\u001b[39;1mCheck your dependencies Python requirement\u001b[39;22m: The Python requirement can be specified via the `\u001b[39;1mpython\u001b[39;22m` or `\u001b[39;1mmarkers\u001b[39;22m` properties\n",
      "    \n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.12\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.11\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.11\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.12\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.12\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.12\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.12\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.12\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.11\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.11\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.11\"\u001b[39m\n",
      "    For \u001b[39;1mtensorflow-io-gcs-filesystem\u001b[39;22m, a possible solution would be to set the `\u001b[39;1mpython\u001b[39;22m` property to \u001b[33m\">=3.10,<3.11\"\u001b[39m\n",
      "\n",
      "    \u001b[34mhttps://python-poetry.org/docs/dependency-specification/#python-restricted-dependencies\u001b[39m,\n",
      "    \u001b[34mhttps://python-poetry.org/docs/dependency-specification/#using-environment-markers\u001b[39m\n",
      "Requirement already satisfied: tensorflow in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached numpy-1.24.3-cp310-cp310-macosx_11_0_arm64.whl (13.9 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/elijahlueders/Library/Caches/pypoetry/virtualenvs/nlp-local-JrpFJHsc-py3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, numpy, keras\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.5\n",
      "    Uninstalling urllib3-2.0.5:\n",
      "      Successfully uninstalled urllib3-2.0.5\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "Successfully installed keras-2.13.1 numpy-1.24.3 typing-extensions-4.5.0 urllib3-1.26.18\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!poetry add datasets keras tensorflow transformers\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Sentence\n",
    "\n",
    "*The flights the airline was cancelling were full*\n",
    "\n",
    "Suppose we have generated `The flights the airline`\n",
    "* `was` is a good next choice\n",
    "   - `airline` has context for `was` vs. `were`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have generated `The flights the airline was cancelling`\n",
    "* `was`/`were` depends on `flights`\n",
    "* much more distance information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Vanishing Gradient\n",
    "\n",
    "The *vanishing gradient* is a common problem in deep neural networks\n",
    "\n",
    "If weights are < 1, they will get smaller and smaller each node they have to pass through - causing them to have little or not effect\n",
    "\n",
    "Happens during training too - error/loss is propogated backwards through the network proportional to the weights on each edge\n",
    "* earlier edges in the network are left with little error to use in adjusting weights\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/vanishing_gradient.png\">\n",
    "</div>\n",
    "\n",
    "image source: https://www.researchgate.net/figure/A-visualization-of-the-vanishing-gradient-problem-using-the-architecture-depicted-in_fig8_277603865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks try to address the vanishing gradient through\n",
    "* removing unneeded information from the context\n",
    "* adding information likely needed later\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LSTM_node.png\", width=700>\n",
    "</div>\n",
    "\n",
    "image_source: SLP Fig. 19.3, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How does it do this?\n",
    "\n",
    "* Explicit **context layer** $c_t$\n",
    "* neural **gates** that control the flow of information through the layer\n",
    "    - $f$ - the **forget gate** - delete info from context that is no longer needed\n",
    "    - $g$ - basic extraction of info from previous hidden state\n",
    "    - $i$ - the **add gate** - select info to add to current context\n",
    "    - $o$ - the **output gate** - decide what info is needed for current hidden state\n",
    "    \n",
    "<div>\n",
    "    <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/hadamard_product.png\"></td><td style=\"text-align: left;\"><b>Hadamard product:</b> bitwise multiplication</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sigmoid.png\"></td><td style=\"text-align: left;\"><b>Sigmoid activation:</b> pushes everything to 0 or 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/tanh.png\"></td><td style=\"text-align: left;\"><b>Hyperbolic tangent activation:</b> pushes to 0 or 1, more like identity at the origin</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "</div>\n",
    "\n",
    "Combining sigmoid with ⊙ has the effect of *masking* out information removing some, leaving others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated Recurrent Unit\n",
    "\n",
    "A **Gated Recurrent Unit** is a popular unit similar to LSTM, except more lightweight\n",
    "* no output gate\n",
    "* no context vector\n",
    "\n",
    "Performance is often similar, but fewer parameters\n",
    "* faster\n",
    "* less memory\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/RNN-vs-LSTM-vs-GRU.png\", width=700>\n",
    "</div>\n",
    "\n",
    "image source: http://dprogrammer.org/rnn-lstm-gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's work with some data\n",
    "\n",
    "We'll do something that should be an easier learning problem: text classification with a recurrent network\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/RNN_classification.png\" width=700>\n",
    "</div>\n",
    "\n",
    "image source: SLP Fig. 9.8, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0b751093943ba9f22a398c37f09b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4fcf5891754d2ba5705268af0fa219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e547afe800e4e33a6a419dec9cfe838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f161d20bae194e85b6bbb85523ee8ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1d27527e74c45b5d93c93fc23e567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f884550330b64a0cb7f8e9402b3c7ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c16f38454f4a9db08bf3ef3eecb1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment these to work with a subset of the data\n",
    "data_subset_text, _, data_subset_label, _ = train_test_split(dataset[\"train\"][\"text\"],dataset[\"train\"][\"label\"],train_size=5000) \n",
    "train_data_text,  test_data_text, train_data_label, test_data_label = train_test_split(data_subset_text, data_subset_label,test_size = 0.2)\n",
    "\n",
    "# uncomment these to use the full original data\n",
    "# train_data_text = dataset[\"train\"][\"text\"]\n",
    "# train_data_label = dataset[\"train\"][\"label\"]\n",
    "# test_data_text = dataset[\"test\"][\"text\"]\n",
    "# test_data_label = dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is one of those films the British Lottery Fund wastes its money on. The main problem is a rambling script which gets nowhere. The characters are not interesting, the story is conventional and insipid, the only thing of interest is the location: the city of Genoa (Genova in Italian). Having only a superficial acquaintance with Genoa, I had no idea of the intricate alleyways of its Old Town, and that the city was so interesting. I had thought Genoa was dull. I am delighted to say that I have been proved wrong. So from the travelogue point of view, this film has interest. The film contains one splendid performance, by a little girl named Perla Haney-Jardine. She has already made seven films despite being only 12, so she seems determined upon a career as an actress, and judging by her performance in this film, she should go far, as she is a natural and has a great deal of talent. Colin Firth, a reliable and professional actor, was on hand for the filming and when asked to be earnest, he was earnest, and when asked to be anguished, he was anguished. But somebody forgot to give him any worthwhile dialogue. The script is a total shambles. Catherine Keener does exceptionally well in a supporting role, and showing sympathy comes naturally to her, so that everybody would like to have her around (I would like to tell her every time I feel a cold coming on, as I know she would get me a soothing hot drink). So there we have it: Genoa's fascinating narrow alleys, an interesting little girl, and a sympathetic woman. Forget the rest. The older sister played by Willa Holland is such a disgusting character that the fact that the young actress does a good job of being repellent is not exactly the kind of acting tribute she would like to hear, I suspect. The notion that this family go off to Genoa to forget the unfortunate death of the mother is so trite that if we have another film like that, all dead mothers have a right to complain at being exploited. If Michael Winterbottom wanted to make a film about how interesting the old portion of Genoa is, why didn't he just go to the BBC and say he wanted to make a travel film with some mindless celebrity presenter? Why waste money on a feature film which is nothing but a vanity project of idle and meandering vacuity?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#printing out a sample review\n",
    "print( train_data_text[125] )\n",
    "print( train_data_label[125] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SimpleRNN, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "pad_length = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) #only keep the 10000 most common words\n",
    "tokenizer.fit_on_texts(train_data_text)\n",
    "tokenized_train_data = tokenizer.texts_to_sequences(train_data_text)\n",
    "processed_train_data = pad_sequences(tokenized_train_data,maxlen=pad_length, padding='pre')\n",
    "tokenized_test_data = tokenizer.texts_to_sequences(test_data_text)\n",
    "processed_test_data = pad_sequences(tokenized_test_data,maxlen=pad_length, padding='pre')\n",
    "\n",
    "train_target = np.array(train_data_label)\n",
    "test_target = np.array(test_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** I originally had tried `padding='post'` which led to bad results\n",
    "* having a bunch of 0s at the end of a sequence is really bad when you are only sending the last output to the next layer\n",
    "* in general, we shouldn't be using post-padding with recurrent networks\n",
    " - unfortunately, this doesn't seem to be the problem with our encoder-decoder example, but it is still worth going back and fixing if you want to keep working with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple LSTM-based architecture\n",
    "\n",
    "Since this is a binary classification problem, we can use a sigmoid activation and `binary_crossentropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=pad_length))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(hidden_layer_size))\n",
    "model.add(SimpleRNN(hidden_layer_size))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 50)           500000    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 500, 50)           0         \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               15100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 515201 (1.97 MB)\n",
      "Trainable params: 515201 (1.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "63/63 [==============================] - 6s 90ms/step - loss: 0.6988 - accuracy: 0.5138 - val_loss: 0.6869 - val_accuracy: 0.5420\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 6s 89ms/step - loss: 0.6246 - accuracy: 0.6977 - val_loss: 0.7023 - val_accuracy: 0.5230\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 6s 89ms/step - loss: 0.5971 - accuracy: 0.6867 - val_loss: 0.6697 - val_accuracy: 0.5880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a5bd0100>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(processed_train_data,\n",
    "          train_target,\n",
    "          epochs = 3,\n",
    "          batch_size = 64,\n",
    "          validation_data=(processed_test_data,test_target) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "The source I got this code from (https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ ) included Dropout layers - you can try uncommenting them and see what it does.\n",
    "\n",
    "It's also equivalent to writing\n",
    "\n",
    "`model.add(LSTM(hidden_layer_size), dropout=0.2, recurrent_dropout=0.2)`\n",
    "\n",
    "Do some searching and see what you can find out about what dropout layers do and why people use them. Discuss your findings with your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #007acc; color: white; border-radius: 10px; padding: 10px; width: fit-content; max-width: 70%; font-size: 1rem;\">\n",
    "    Does dropout help? No, it was about the same with the current training\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Run an experiment: What is the difference between using `SimpleRNN` and `LSTM` with this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Do one of the following:\n",
    "\n",
    "1. Redo your experiment with another classification dataset. Choose something with more than 2 classes - this will be good practice is understanding the difference you need to make to the model and data prep. Describe your data and results as usual.\n",
    "    * I also suggest including a GRU layer in your experiment as well: https://keras.io/api/layers/recurrent_layers/gru/\n",
    "\n",
    "2. Edit the Encoder-Decoder code from last time to use LSTM or GRU.\n",
    "    * Note that since LSTM returns both a context and hidden state, you will get an output, a hidden state, and context returned from the LSTM layer (instead of just the output and state). It will look something like\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = LSTM(100, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_rnn(enc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you will pass both state_h, state_c as the *context vector* which is the initial state for the decoder. See the source from last time to flesh out the example: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
